{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/zero_shot/name_def/name_scores.csv\n",
      "f1 score file found for name\n",
      "outputs/validation/Chinese/3.5/zero_shot/name_def/name.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/zero_shot/name_def/specialty_scores.csv\n",
      "f1 score file found for specialty\n",
      "outputs/validation/Chinese/3.5/zero_shot/name_def/specialty.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/zero_shot/name_def/positivity_scores.csv\n",
      "f1 score file found for positivity\n",
      "outputs/validation/Chinese/3.5/zero_shot/name_def/positivity.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/zero_shot/name_def/culture_scores.csv\n",
      "f1 score file found for culture\n",
      "outputs/validation/Chinese/3.5/zero_shot/name_def/culture.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/zero_shot/name_def/location_scores.csv\n",
      "f1 score file found for location\n",
      "outputs/validation/Chinese/3.5/zero_shot/name_def/location.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/zero_shot/name_def/ambiance_scores.csv\n",
      "f1 score file found for ambiance\n",
      "outputs/validation/Chinese/3.5/zero_shot/name_def/ambiance.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/zero_shot/name_def/creative_scores.csv\n",
      "f1 score file found for creative\n",
      "outputs/validation/Chinese/3.5/zero_shot/name_def/creative.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/zero_shot/name_def/romanized_scores.csv\n",
      "NO f1 score file found for romanized\n",
      "outputs/validation/Chinese/3.5/zero_shot/name_def/romanized.csv\n",
      "category Romanized doesn't exist\n",
      "category romanized doesn't exist\n",
      "df not found\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/lexicon_based/name_scores.csv\n",
      "f1 score file found for name\n",
      "outputs/validation/Chinese/3.5/few_shot/lexicon_based/name.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/lexicon_based/specialty_scores.csv\n",
      "f1 score file found for specialty\n",
      "outputs/validation/Chinese/3.5/few_shot/lexicon_based/specialty.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/lexicon_based/positivity_scores.csv\n",
      "f1 score file found for positivity\n",
      "outputs/validation/Chinese/3.5/few_shot/lexicon_based/positivity.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/lexicon_based/culture_scores.csv\n",
      "f1 score file found for culture\n",
      "outputs/validation/Chinese/3.5/few_shot/lexicon_based/culture.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/lexicon_based/location_scores.csv\n",
      "f1 score file found for location\n",
      "outputs/validation/Chinese/3.5/few_shot/lexicon_based/location.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/lexicon_based/ambiance_scores.csv\n",
      "f1 score file found for ambiance\n",
      "outputs/validation/Chinese/3.5/few_shot/lexicon_based/ambiance.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/lexicon_based/creative_scores.csv\n",
      "f1 score file found for creative\n",
      "outputs/validation/Chinese/3.5/few_shot/lexicon_based/creative.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/lexicon_based/romanized_scores.csv\n",
      "NO f1 score file found for romanized\n",
      "outputs/validation/Chinese/3.5/few_shot/lexicon_based/romanized.csv\n",
      "category Romanized doesn't exist\n",
      "category romanized doesn't exist\n",
      "df not found\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/rule_based/name_scores.csv\n",
      "f1 score file found for name\n",
      "outputs/validation/Chinese/3.5/few_shot/rule_based/name.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/rule_based/specialty_scores.csv\n",
      "f1 score file found for specialty\n",
      "outputs/validation/Chinese/3.5/few_shot/rule_based/specialty.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/rule_based/positivity_scores.csv\n",
      "f1 score file found for positivity\n",
      "outputs/validation/Chinese/3.5/few_shot/rule_based/positivity.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/rule_based/culture_scores.csv\n",
      "f1 score file found for culture\n",
      "outputs/validation/Chinese/3.5/few_shot/rule_based/culture.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/rule_based/location_scores.csv\n",
      "f1 score file found for location\n",
      "outputs/validation/Chinese/3.5/few_shot/rule_based/location.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/rule_based/ambiance_scores.csv\n",
      "f1 score file found for ambiance\n",
      "outputs/validation/Chinese/3.5/few_shot/rule_based/ambiance.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/rule_based/creative_scores.csv\n",
      "f1 score file found for creative\n",
      "outputs/validation/Chinese/3.5/few_shot/rule_based/creative.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/Chinese/3.5/few_shot/rule_based/romanized_scores.csv\n",
      "NO f1 score file found for romanized\n",
      "outputs/validation/Chinese/3.5/few_shot/rule_based/romanized.csv\n",
      "category Romanized doesn't exist\n",
      "category romanized doesn't exist\n",
      "df not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanxiliu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# !python3 -m pip install -U scikit-learn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# this function calculates all the scores - accuracy, f1, precision, recall, and returns them in a list\n",
    "def get_accuracy(column_cat, model,file_cat='',lang='en',process='validation'):\n",
    "    try:\n",
    "        if lang=='cn':\n",
    "            path = 'outputs/'+process+'/Chinese/'+model+'/'+file_cat+'.csv'\n",
    "            print(path)\n",
    "            df_pred = pd.read_csv(path)\n",
    "            if process == 'validation':\n",
    "                labels = '../data_cleaning/output/validation_cn.csv'\n",
    "            else:\n",
    "                labels='../data_cleaning/output/test_cn.csv'\n",
    "        else:\n",
    "            path = 'outputs/'+process+'/English/'+model+'/'+file_cat+'.csv'\n",
    "            print(path)\n",
    "            df_pred = pd.read_csv(path)\n",
    "            if process == 'validation':\n",
    "                labels = '../data_cleaning/output/validation_en.csv'\n",
    "            else:\n",
    "                labels='../data_cleaning/output/test_en.csv'\n",
    "#         print('here1')\n",
    "        df_true = pd.read_csv(labels)\n",
    "#         print('here2')\n",
    "        y_pred = df_pred[column_cat]\n",
    "        y_true = df_true[column_cat]\n",
    "        output = df_true[column_cat].value_counts()\n",
    "#         print('here3')\n",
    "        accuracy = round(100*accuracy_score(y_true, y_pred), 2)\n",
    "        scores = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "#         print('here4')\n",
    "        return [column_cat,accuracy, max(output[0], output[1])/len(y_pred), round(100*scores[0],2), round(100*scores[1],2), round(100*scores[2],2)]#overlap/len(y_pred)\n",
    "    except:\n",
    "        print(\"category\",column_cat,\"doesn't exist\")\n",
    "        return None\n",
    "\n",
    "# all_scores_file is the name of the csv file that contains all the scores of all files\n",
    "# criteria is the type of category we're looking into\n",
    "# model_num is the model's version\n",
    "# pred is the 1-D array with the predicted values\n",
    "# prompt is the file path of the prompt file\n",
    "def add_score(all_scores_file, model_num,column_cat, file_cat, prompt,lang, clear=False, process='validation'):\n",
    "    try:\n",
    "        df_all = pd.read_csv(all_scores_file)\n",
    "#         df_f1 = pd.read_csv(f1_file)\n",
    "        print(\"f1 score file found for\",file_cat)\n",
    "    except:\n",
    "        df_all = pd.DataFrame(columns=['file', 'accuracy','majority_baseline','precision','recall','f1','prompt'])\n",
    "#         df_f1 = pd.DataFrame(columns=['category','f1'])\n",
    "        print(\"NO f1 score file found for\",file_cat)\n",
    "    try:\n",
    "        # gets all the scores and store the scores into all_scores_file\n",
    "        if clear:\n",
    "            df_all = df_all.iloc[0:0]\n",
    "        if \"llama\" in all_scores_file:\n",
    "            model_num = \"llama/\"+model_num\n",
    "        result_all = get_accuracy(column_cat, model_num,file_cat,lang, process)\n",
    "        result_all.append(prompt)\n",
    "#         else:\n",
    "        df_all.loc[len(df_all.index)] = result_all\n",
    "        df_all.to_csv(all_scores_file, index = False, encoding='utf-8')\n",
    "        \n",
    "        # gets the f1 score and store it into f1_file. The purpose of this is to make it easier to compile all the f1 data.\n",
    "#         result_f1 = [result_all[0]]+[result_all[5]]\n",
    "#         df_f1.loc[len(df_f1.index)] = result_f1\n",
    "#         df_f1.to_csv(f1_file, index = False, encoding='utf-8')\n",
    "        \n",
    "    except: \n",
    "        print(\"category\",file_cat,\"doesn't exist\")\n",
    "\n",
    "\n",
    "file = ['name','specialty','positivity','culture','location','ambiance', 'creative','romanized']\n",
    "col = ['Personal_Name','Specialty','Positivity','Culture','Location','Ambiance', 'Pun_Creative','Romanized']\n",
    "# cat=['creative']\n",
    "# crit = ['Pun_Creative']\n",
    "\n",
    "# run this to get the scores\n",
    "# pred and crit are the different categories we're calculating scores for. \n",
    "def score_to_file(file, model_n,model_store,col, lang='en', clear=False,model=\"GPT\", process='validation'):\n",
    "\n",
    "    all_dfs=[]\n",
    "    for i in range(len(file)):\n",
    "        file_cat = file[i]\n",
    "        column_cat = col[i]\n",
    "        if lang=='en':\n",
    "            store = 'scores/'+process+'/'+model+'/English/'+model_n+'/'+file_cat+'_scores.csv'\n",
    "            print(\"store:--------------------------------\")\n",
    "            print(store)\n",
    "            prompt = 'prompt_en_'+file_cat\n",
    "        elif lang=='cn':\n",
    "            store = 'scores/'+process+'/'+model+'/Chinese/'+model_n+'/'+file_cat+'_scores.csv'\n",
    "            prompt = 'prompt_cn_'+file_cat\n",
    "            print(\"store:--------------------------------\")\n",
    "            print(store)\n",
    "        add_score(store,model_n, column_cat,file_cat,prompt,lang,clear,process)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(store)\n",
    "            all_dfs.append(df)\n",
    "        except:\n",
    "            print(\"df not found\")\n",
    "        \n",
    "    if lang=='cn':\n",
    "        store_all = 'scores/'+process+'/'+model+'/Chinese/'+model_n+'/'+lang+'_'+model_store+'_all_scores.csv'\n",
    "    else:\n",
    "        store_all = 'scores/'+process+'/'+model+'/English/'+model_n+'/'+lang+'_'+model_store+'_all_scores.csv'\n",
    "        print(store_all)\n",
    "    all_scores = pd.concat(all_dfs)\n",
    "    all_scores.to_csv(store_all, index = False, encoding='utf-8')\n",
    "        \n",
    "    \n",
    "\n",
    "#####GPT 3.5 scores calculation English\n",
    "# score_to_file(file, '3.5/zero_shot/name_def','3.5_zero_shot_name_def',col, clear='True')\n",
    "# score_to_file(file, '3.5/few_shot/lexicon_based','3.5_few_shot_lexicon_based',col,clear='True')\n",
    "# score_to_file(file, '3.5/few_shot/rule_based','3.5_few_shot_rule_based', col,clear='True')\n",
    "\n",
    "#####GPT 3.5 scores calculation Chinese\n",
    "score_to_file(file, '3.5/zero_shot/name_def','3.5_zero_shot_name_def',col, lang='cn',clear='True')\n",
    "score_to_file(file, '3.5/few_shot/lexicon_based','3.5_few_shot_lexicon_based',col,lang='cn',clear='True')\n",
    "score_to_file(file, '3.5/few_shot/rule_based','3.5_few_shot_rule_based', col,lang='cn',clear='True')\n",
    "\n",
    "##### Llama 2 scores calculation\n",
    "# score_to_file(file, 'zero_shot/only_name','llama_zero_shot_only_name',col,model=\"llama\")\n",
    "# score_to_file(file, 'zero_shot/name_def','llama_zero_shot_name_def',col,model=\"llama\")\n",
    "# score_to_file(file, 'few_shot/lexicon_based','llama_few_shot_lexicon_based',col,model=\"llama\")\n",
    "# score_to_file(file, 'few_shot/rule_based','llama_few_shot_rule_based',col,model=\"llama\")\n",
    "\n",
    "# score_to_file(cat, '4/zero_shot/only_name','4_zero_shot_only_name',crit,'en')\n",
    "# score_to_file(cat, '4/zero_shot/name_def','4_zero_shot_name_def',crit,'en')\n",
    "# score_to_file(cat, '3.5/few_shot/lexicon_based','3.5_few_shot_lexicon_based',crit,'en')\n",
    "# score_to_file(cat, '4/few_shot/lexicon_based','4_few_shot_lexicon_based',crit,'en')\n",
    "# score_to_file(cat, '4/few_shot/rule_based','4_few_shot_rule_based',crit)\n",
    "\n",
    "# score_to_file(cat, '3.5/zero_shot/only_name','3.5_zero_shot_only_name',crit,'cn')\n",
    "# score_to_file(cat, '3.5/zero_shot/name_def','3.5_zero_shot_name_def',crit,'cn')\n",
    "# score_to_file(cat, '3.5/few_shot/lexicon_based','3.5_few_shot_lexicon_based',crit,'cn')\n",
    "# score_to_file(cat, '3.5/few_shot/rule_based','3.5_few_shot_rule_based',crit,'cn')\n",
    "\n",
    "# score_to_file(cat, '4/zero_shot/only_name','4_zero_shot_only_name',crit,'cn')\n",
    "# score_to_file(cat, '4/zero_shot/name_def','4_zero_shot_name_def',crit,'cn')\n",
    "# score_to_file(cat, '4/few_shot/lexicon_based','4_few_shot_lexicon_based',crit,'cn')\n",
    "# score_to_file(cat, '3.5/few_shot/rule_based','3.5_few_shot_rule_based',crit,'cn') * recalculated\n",
    "# add_score('scores/GPT/English/creative_scores.csv','Pun_Creative','creative','training_validation/training_en.csv','prompt_en_creative_1')\n",
    "# add_score(store,criteria, output, labels,prompt)\n",
    "# get_accuracy(criteria,output, labels)\n",
    "\n",
    "# score_to_file(cat, 'zero_shot/only_name','llama_zero_shot_only_name',crit,model=\"llama\")\n",
    "# score_to_file(cat, 'zero_shot/name_def','llama_zero_shot_name_def',crit,model=\"llama\")\n",
    "# score_to_file(cat, 'few_shot/lexicon_based','llama_few_shot_lexicon_based',crit,model=\"llama\")\n",
    "# score_to_file(cat, 'few_shot/rule_based','llama_few_shot_rule_based',crit,model=\"llama\")\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivity\n",
      "0    270\n",
      "1    149\n",
      "Name: count, dtype: int64\n",
      "0.6443914081145584\n"
     ]
    }
   ],
   "source": [
    "# file is the name of the csv file that contains all the scores of all files\n",
    "# criteria is the type of category we're looking into\n",
    "# pred is the 1-D array with the predicted values\n",
    "# true is the original given labels\n",
    "# prompt is the file path of the prompt file\n",
    "def add_score(file, criteria, pred, true, prompt):\n",
    "    df = pd.read_csv(file)\n",
    "    # df['prompt']=''\n",
    "    result = get_accuracy(criteria, pred, true)\n",
    "    # print(result)\n",
    "    result.append(prompt)\n",
    "    # print('===============================')\n",
    "    # print(result)\n",
    "    df.loc[len(df.index)] = result\n",
    "    df.to_csv(file, index = False, encoding='utf-8')\n",
    "\n",
    "add_score(store,criteria, file1, file2,prompt)\n",
    "# df=pd.read_csv('./scores/positivity_scores.csv')\n",
    "# print(df['f1'])\n",
    "# positivity = pd.DataFrame(columns=['file', 'accuracy','majority_baseline','precision','recall','f1','prompt'])\n",
    "# positivity.loc[len(positivity.index)] = result\n",
    "# positivity.to_csv('./scores/positivity_scores.csv', index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['positivity', 'specialty', 'name', 'location', 'creative', 'ambiance','culture']\n",
    "for name in cat:\n",
    "    df = pd.DataFrame(columns=['file','accuracy','majority_baseline','precision','recall','f1','prompt'])\n",
    "    df.to_csv('scores/Chinese/'+name+'_scores.csv',index=False,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
