{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store:--------------------------------\n",
      "scores/validation/GPT/English/3.5/zero_shot/name_def/name_scores.csv\n",
      "f1 score file found for name\n",
      "outputs/validation/English/3.5/zero_shot/name_def/name.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/English/3.5/zero_shot/name_def/specialty_scores.csv\n",
      "f1 score file found for specialty\n",
      "outputs/validation/English/3.5/zero_shot/name_def/specialty.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/English/3.5/zero_shot/name_def/positivity_scores.csv\n",
      "f1 score file found for positivity\n",
      "outputs/validation/English/3.5/zero_shot/name_def/positivity.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/English/3.5/zero_shot/name_def/culture_scores.csv\n",
      "f1 score file found for culture\n",
      "outputs/validation/English/3.5/zero_shot/name_def/culture.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/English/3.5/zero_shot/name_def/location_scores.csv\n",
      "f1 score file found for location\n",
      "outputs/validation/English/3.5/zero_shot/name_def/location.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/English/3.5/zero_shot/name_def/ambiance_scores.csv\n",
      "f1 score file found for ambiance\n",
      "outputs/validation/English/3.5/zero_shot/name_def/ambiance.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/English/3.5/zero_shot/name_def/creative_scores.csv\n",
      "f1 score file found for creative\n",
      "outputs/validation/English/3.5/zero_shot/name_def/creative.csv\n",
      "store:--------------------------------\n",
      "scores/validation/GPT/English/3.5/zero_shot/name_def/romanized_scores.csv\n",
      "f1 score file found for romanized\n",
      "outputs/validation/English/3.5/zero_shot/name_def/romanized.csv\n",
      "scores/validation/GPT/English/3.5/zero_shot/name_def/en_3.5_zero_shot_name_def_all_scores.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanxiliu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xlsxwriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 121\u001b[0m\n\u001b[1;32m    114\u001b[0m     all_scores\u001b[38;5;241m.\u001b[39mto_excel(store_all, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlsxwriter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#     all_scores.to_csv(store_all, index = False, encoding='utf-8')\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \n\u001b[1;32m    117\u001b[0m     \n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# score_to_file(cat, crit,'cn')\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# score_to_file(cat, '3.5/zero_shot/only_name','3.5_zero_shot_only_name',crit,'en')\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m score_to_file(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.5/zero_shot/name_def\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.5_zero_shot_name_def\u001b[39m\u001b[38;5;124m'\u001b[39m,col, clear\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    122\u001b[0m score_to_file(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.5/few_shot/lexicon_based\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.5_few_shot_lexicon_based\u001b[39m\u001b[38;5;124m'\u001b[39m,col,clear\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    123\u001b[0m score_to_file(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.5/few_shot/rule_based\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.5_few_shot_rule_based\u001b[39m\u001b[38;5;124m'\u001b[39m, col,clear\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 114\u001b[0m, in \u001b[0;36mscore_to_file\u001b[0;34m(file, model_n, model_store, col, lang, clear, model, process)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(store_all)\n\u001b[1;32m    113\u001b[0m all_scores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_dfs)\n\u001b[0;32m--> 114\u001b[0m all_scores\u001b[38;5;241m.\u001b[39mto_excel(store_all, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlsxwriter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:2252\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options)\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[1;32m   2241\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[1;32m   2242\u001b[0m     df,\n\u001b[1;32m   2243\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2250\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[1;32m   2251\u001b[0m )\n\u001b[0;32m-> 2252\u001b[0m formatter\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m   2253\u001b[0m     excel_writer,\n\u001b[1;32m   2254\u001b[0m     sheet_name\u001b[38;5;241m=\u001b[39msheet_name,\n\u001b[1;32m   2255\u001b[0m     startrow\u001b[38;5;241m=\u001b[39mstartrow,\n\u001b[1;32m   2256\u001b[0m     startcol\u001b[38;5;241m=\u001b[39mstartcol,\n\u001b[1;32m   2257\u001b[0m     freeze_panes\u001b[38;5;241m=\u001b[39mfreeze_panes,\n\u001b[1;32m   2258\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m   2259\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2260\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/formats/excel.py:934\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[1;32m    930\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;66;03m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001b[39;00m\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;66;03m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m     writer \u001b[38;5;241m=\u001b[39m ExcelWriter(  \u001b[38;5;66;03m# type: ignore[abstract]\u001b[39;00m\n\u001b[1;32m    935\u001b[0m         writer, engine\u001b[38;5;241m=\u001b[39mengine, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m    936\u001b[0m     )\n\u001b[1;32m    937\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_xlsxwriter.py:192\u001b[0m, in \u001b[0;36mXlsxWriter.__init__\u001b[0;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# Use the xlsxwriter module as the Excel writer.\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxlsxwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[1;32m    194\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xlsxwriter'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# !python3 -m pip install -U scikit-learn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# this function calculates all the scores - accuracy, f1, precision, recall, and returns them in a list\n",
    "def get_accuracy(column_cat, model,file_cat='',lang='en',process='validation'):\n",
    "    try:\n",
    "        if lang=='cn':\n",
    "            path = 'outputs/'+process+'/Chinese/'+model+'/'+file_cat+'.csv'\n",
    "            print(path)\n",
    "            df_pred = pd.read_csv(path)\n",
    "            if process == 'validation':\n",
    "                labels = '../data_cleaning/output/validation_cn.csv'\n",
    "            else:\n",
    "                labels='../data_cleaning/output/test_cn.csv'\n",
    "        else:\n",
    "            path = 'outputs/'+process+'/English/'+model+'/'+file_cat+'.csv'\n",
    "            print(path)\n",
    "            df_pred = pd.read_csv(path)\n",
    "            if process == 'validation':\n",
    "                labels = '../data_cleaning/output/validation_en.csv'\n",
    "            else:\n",
    "                labels='../data_cleaning/output/test_en.csv'\n",
    "#         print('here1')\n",
    "        df_true = pd.read_csv(labels)\n",
    "#         print('here2')\n",
    "        y_pred = df_pred[column_cat]\n",
    "        y_true = df_true[column_cat]\n",
    "        output = df_true[column_cat].value_counts()\n",
    "#         print('here3')\n",
    "        accuracy = round(100*accuracy_score(y_true, y_pred), 2)\n",
    "        scores = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "#         print('here4')\n",
    "        return [column_cat,accuracy, max(output[0], output[1])/len(y_pred), round(100*scores[0],2), round(100*scores[1],2), round(100*scores[2],2)]#overlap/len(y_pred)\n",
    "    except:\n",
    "        print(\"category\",column_cat,\"doesn't exist\")\n",
    "        return None\n",
    "\n",
    "# all_scores_file is the name of the csv file that contains all the scores of all files\n",
    "# criteria is the type of category we're looking into\n",
    "# model_num is the model's version\n",
    "# pred is the 1-D array with the predicted values\n",
    "# prompt is the file path of the prompt file\n",
    "def add_score(all_scores_file, model_num,column_cat, file_cat, prompt,lang, clear=False, process='validation'):\n",
    "    try:\n",
    "        df_all = pd.read_csv(all_scores_file)\n",
    "#         df_f1 = pd.read_csv(f1_file)\n",
    "        print(\"f1 score file found for\",file_cat)\n",
    "    except:\n",
    "        df_all = pd.DataFrame(columns=['file', 'accuracy','majority_baseline','precision','recall','f1','prompt'])\n",
    "#         df_f1 = pd.DataFrame(columns=['category','f1'])\n",
    "        print(\"NO f1 score file found for\",file_cat)\n",
    "    try:\n",
    "        # gets all the scores and store the scores into all_scores_file\n",
    "        if clear:\n",
    "            df_all = df_all.iloc[0:0]\n",
    "        if \"llama\" in all_scores_file:\n",
    "            model_num = \"llama/\"+model_num\n",
    "        result_all = get_accuracy(column_cat, model_num,file_cat,lang, process)\n",
    "        result_all.append(prompt)\n",
    "#         else:\n",
    "        df_all.loc[len(df_all.index)] = result_all\n",
    "        df_all.to_csv(all_scores_file, index = False, encoding='utf-8')\n",
    "        \n",
    "        # gets the f1 score and store it into f1_file. The purpose of this is to make it easier to compile all the f1 data.\n",
    "#         result_f1 = [result_all[0]]+[result_all[5]]\n",
    "#         df_f1.loc[len(df_f1.index)] = result_f1\n",
    "#         df_f1.to_csv(f1_file, index = False, encoding='utf-8')\n",
    "        \n",
    "    except: \n",
    "        print(\"category\",file_cat,\"doesn't exist\")\n",
    "\n",
    "\n",
    "file = ['name','specialty','positivity','culture','location','ambiance', 'creative','romanized']\n",
    "col = ['Personal_Name','Specialty','Positivity','Culture','Location','Ambiance', 'Pun_Creative','Romanized']\n",
    "# cat=['creative']\n",
    "# crit = ['Pun_Creative']\n",
    "\n",
    "# run this to get the scores\n",
    "# pred and crit are the different categories we're calculating scores for. \n",
    "def score_to_file(file, model_n,model_store,col, lang='en', clear=False,model=\"GPT\", process='validation'):\n",
    "\n",
    "    all_dfs=[]\n",
    "    for i in range(len(file)):\n",
    "        file_cat = file[i]\n",
    "        column_cat = col[i]\n",
    "        if lang=='en':\n",
    "            store = 'scores/'+process+'/'+model+'/English/'+model_n+'/'+file_cat+'_scores.csv'\n",
    "            print(\"store:--------------------------------\")\n",
    "            print(store)\n",
    "            prompt = 'prompt_en_'+file_cat\n",
    "        elif lang=='cn':\n",
    "            store = 'scores/'+process+'/'+model+'/Chinese/'+model_n+'/'+file_cat+'_scores.csv'\n",
    "            prompt = 'prompt_cn_'+file_cat\n",
    "            print(\"store:--------------------------------\")\n",
    "            print(store)\n",
    "        add_score(store,model_n, column_cat,file_cat,prompt,lang,clear,process)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(store)\n",
    "            all_dfs.append(df)\n",
    "        except:\n",
    "            print(\"df not found\")\n",
    "        \n",
    "    if lang=='cn':\n",
    "        store_all = 'scores/'+process+'/'+model+'/Chinese/'+model_n+'/'+lang+'_'+model_store+'_all_scores.csv'\n",
    "    else:\n",
    "        store_all = 'scores/'+process+'/'+model+'/English/'+model_n+'/'+lang+'_'+model_store+'_all_scores.csv'\n",
    "        print(store_all)\n",
    "    all_scores = pd.concat(all_dfs)\n",
    "    all_scores.to_csv(store_all, index = False, encoding='utf-8')\n",
    "        \n",
    "    \n",
    "\n",
    "#####GPT 3.5 scores calculation English\n",
    "# score_to_file(file, '3.5/zero_shot/name_def','3.5_zero_shot_name_def',col, clear='True')\n",
    "# score_to_file(file, '3.5/few_shot/lexicon_based','3.5_few_shot_lexicon_based',col,clear='True')\n",
    "# score_to_file(file, '3.5/few_shot/rule_based','3.5_few_shot_rule_based', col,clear='True')\n",
    "\n",
    "#####GPT 3.5 scores calculation Chinese\n",
    "score_to_file(file, '3.5/zero_shot/name_def','3.5_zero_shot_name_def',col, lang='cn',clear='True')\n",
    "score_to_file(file, '3.5/few_shot/lexicon_based','3.5_few_shot_lexicon_based',col,lang='cn',clear='True')\n",
    "score_to_file(file, '3.5/few_shot/rule_based','3.5_few_shot_rule_based', col,lang='cn',clear='True')\n",
    "\n",
    "##### Llama 2 scores calculation\n",
    "# score_to_file(file, 'zero_shot/only_name','llama_zero_shot_only_name',col,model=\"llama\")\n",
    "# score_to_file(file, 'zero_shot/name_def','llama_zero_shot_name_def',col,model=\"llama\")\n",
    "# score_to_file(file, 'few_shot/lexicon_based','llama_few_shot_lexicon_based',col,model=\"llama\")\n",
    "# score_to_file(file, 'few_shot/rule_based','llama_few_shot_rule_based',col,model=\"llama\")\n",
    "\n",
    "# score_to_file(cat, '4/zero_shot/only_name','4_zero_shot_only_name',crit,'en')\n",
    "# score_to_file(cat, '4/zero_shot/name_def','4_zero_shot_name_def',crit,'en')\n",
    "# score_to_file(cat, '3.5/few_shot/lexicon_based','3.5_few_shot_lexicon_based',crit,'en')\n",
    "# score_to_file(cat, '4/few_shot/lexicon_based','4_few_shot_lexicon_based',crit,'en')\n",
    "# score_to_file(cat, '4/few_shot/rule_based','4_few_shot_rule_based',crit)\n",
    "\n",
    "# score_to_file(cat, '3.5/zero_shot/only_name','3.5_zero_shot_only_name',crit,'cn')\n",
    "# score_to_file(cat, '3.5/zero_shot/name_def','3.5_zero_shot_name_def',crit,'cn')\n",
    "# score_to_file(cat, '3.5/few_shot/lexicon_based','3.5_few_shot_lexicon_based',crit,'cn')\n",
    "# score_to_file(cat, '3.5/few_shot/rule_based','3.5_few_shot_rule_based',crit,'cn')\n",
    "\n",
    "# score_to_file(cat, '4/zero_shot/only_name','4_zero_shot_only_name',crit,'cn')\n",
    "# score_to_file(cat, '4/zero_shot/name_def','4_zero_shot_name_def',crit,'cn')\n",
    "# score_to_file(cat, '4/few_shot/lexicon_based','4_few_shot_lexicon_based',crit,'cn')\n",
    "# score_to_file(cat, '3.5/few_shot/rule_based','3.5_few_shot_rule_based',crit,'cn') * recalculated\n",
    "# add_score('scores/GPT/English/creative_scores.csv','Pun_Creative','creative','training_validation/training_en.csv','prompt_en_creative_1')\n",
    "# add_score(store,criteria, output, labels,prompt)\n",
    "# get_accuracy(criteria,output, labels)\n",
    "\n",
    "# score_to_file(cat, 'zero_shot/only_name','llama_zero_shot_only_name',crit,model=\"llama\")\n",
    "# score_to_file(cat, 'zero_shot/name_def','llama_zero_shot_name_def',crit,model=\"llama\")\n",
    "# score_to_file(cat, 'few_shot/lexicon_based','llama_few_shot_lexicon_based',crit,model=\"llama\")\n",
    "# score_to_file(cat, 'few_shot/rule_based','llama_few_shot_rule_based',crit,model=\"llama\")\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivity\n",
      "0    270\n",
      "1    149\n",
      "Name: count, dtype: int64\n",
      "0.6443914081145584\n"
     ]
    }
   ],
   "source": [
    "# file is the name of the csv file that contains all the scores of all files\n",
    "# criteria is the type of category we're looking into\n",
    "# pred is the 1-D array with the predicted values\n",
    "# true is the original given labels\n",
    "# prompt is the file path of the prompt file\n",
    "def add_score(file, criteria, pred, true, prompt):\n",
    "    df = pd.read_csv(file)\n",
    "    # df['prompt']=''\n",
    "    result = get_accuracy(criteria, pred, true)\n",
    "    # print(result)\n",
    "    result.append(prompt)\n",
    "    # print('===============================')\n",
    "    # print(result)\n",
    "    df.loc[len(df.index)] = result\n",
    "    df.to_csv(file, index = False, encoding='utf-8')\n",
    "\n",
    "add_score(store,criteria, file1, file2,prompt)\n",
    "# df=pd.read_csv('./scores/positivity_scores.csv')\n",
    "# print(df['f1'])\n",
    "# positivity = pd.DataFrame(columns=['file', 'accuracy','majority_baseline','precision','recall','f1','prompt'])\n",
    "# positivity.loc[len(positivity.index)] = result\n",
    "# positivity.to_csv('./scores/positivity_scores.csv', index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['positivity', 'specialty', 'name', 'location', 'creative', 'ambiance','culture']\n",
    "for name in cat:\n",
    "    df = pd.DataFrame(columns=['file','accuracy','majority_baseline','precision','recall','f1','prompt'])\n",
    "    df.to_csv('scores/Chinese/'+name+'_scores.csv',index=False,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
