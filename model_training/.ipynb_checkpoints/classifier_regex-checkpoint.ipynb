{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and preparing the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training', '/Users/nanxiliu/anaconda3/lib/python311.zip', '/Users/nanxiliu/anaconda3/lib/python3.11', '/Users/nanxiliu/anaconda3/lib/python3.11/lib-dynload', '', '/Users/nanxiliu/.local/lib/python3.11/site-packages', '/Users/nanxiliu/anaconda3/lib/python3.11/site-packages', '/Users/nanxiliu/anaconda3/lib/python3.11/site-packages/aeosa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import re\n",
    "import os\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "# !conda install -c conda-forge seqeval\n",
    "import seqeval\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from cjkfuzz import fuzz \n",
    "from cjkfuzz import process\n",
    "import collections\n",
    "import langid\n",
    "\n",
    "df_idioms = pd.read_csv('utilities/chinese_idioms.csv')\n",
    "\n",
    "### English NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "### Chinese NER\n",
    "tokenizer_cn = AutoTokenizer.from_pretrained(\"shibing624/bert4ner-base-chinese\")\n",
    "model_cn = AutoModelForTokenClassification.from_pretrained(\"shibing624/bert4ner-base-chinese\")\n",
    "label_list = ['I-ORG', 'B-LOC', 'O', 'B-ORG', 'I-LOC', 'I-PER', 'B-TIME', 'I-TIME', 'B-PER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = [\"chinese\", \"mandarin\", \"asia\", \"cathay\", \"china\",\"Xi’an\", \"ShanDong\", \"Hunan\", \"beijing\", \"Hong Kong\", \"shanghai\", \"tokyo\", \"canton\"]\n",
    "# culture=['panda']\n",
    "culture=[\"Panda\", \"Dragon\", \"Phoenix\",\"Great Wall\", \"Forbidden City\", \"Golden Gate\",\"Bamboo\", \"Plum\",'lotus','tang','song','han','Jackie Chen','xiaoping']\n",
    "ambiance=['house', 'home', 'kitchen', 'dining room',\"Brother\", \"Uncle\", \"Grandma\", \"Mama\",\"friend\", \"friendship\",\"Hometown\", \"home\",'hut', 'village', 'inn', 'bistro','garden','pavillion','palace']\n",
    "specialty = [\"Duck\", \"Fishball\", \"Sheep\", \"Seafood\", \"Tofu\", \"BBQ\", \"Buffet\", \"Green Tea\", \"Boba Tea\", \"Tea\", \"Dim Sum\", \"Yum Cha\", \"Dumpling\", \"Bao\", \"Egg Roll\", \"Hot Pot\", \"Pot\", \"Noodle\", \"Rice\", \"Noodles\", \"Congee\", \"Porridge\", \"Halal\", \"Veggie\", \"Vegan\", \"Kosher\", \"Wok\", \"Rice\", \"Snacks\"]\n",
    "positivity=['best', 'great', 'perfect', 'elite', 'legendary', 'No.1', 'OK', 'super','Delicious', 'Tasty', 'Yummy', 'Luscious', 'Gourmet', 'Fresh','gold', 'golden', 'silver', 'diamond', 'jade','specialist', 'master', 'artisan','8', 'lucky', 'fortune','Happy', 'Joy', 'Sunny', 'Pleasure', 'Nice','Grand', 'Feast', 'Jumbo','King', 'Empress', 'Empire', 'Regent', 'Royal', 'VIP', 'Dynasty','red']\n",
    "creativity = ['with','to','and','in','out']\n",
    "\n",
    "location_cn = ['中',\"亚\",\"华\",\"北京\",\"香港\",\"上海\",\"川\",\"湘\",\"陕\",\"山东\",\"粤\",\"广东\",\"杭州\",\"南京\",\"西安\",\"重庆\",\"成都\",\"桂林\"]\n",
    "culture_cn=[\"熊猫\",\"龙\",\"凤\",\"长城\",\"天坛\",\"金门\",\"松\",\"鹤\",\"梅花\",\"竹\",\"小平\",\"秦始皇\",\"汉\",\"唐\",\"宋\",\"灯笼\",\"瓷\",\"丁香\",\"荷\",\"菊\",\"功夫\",\"丝绸\"]\n",
    "ambiance_cn=[\"小馆\",\"厨\",\"膳房\",\"轩\",\"楼\",\"阁\",\"亭\",\"坊\",\"榭\",\"堂\",\"居\",\"园\",\"家\",\"宫\",\"庭\",\"村\",\"乡\",\"屋\",\"苑\",\"吧\",\"城\",\"府\",\"堂\",\"屯\",\"邨\",\"庄\",\"湾\"]\n",
    "specialty_cn=[\"烤\",\"羊\",\"牛\",\"猪\",\"鸡\",\"鸭\",\"鹅\",\"海鲜\",\"豆腐\",\"串\",\"自助\",\"茶\",\"点心\",\"包子\",\"饺\",\"锅\",\"煲\",\"饭\",\"粉\",\"粥\",\"面\",\"清真\",\"素食\",\"小吃\",\"馍\",\"烧烤\",\"寿司\",\"腊\",\"卤\",\"辣\",\"饼\",\"烫\",\"烩\",\"鱼\",\"米\",\"渔\",\"汤\"]\n",
    "positivity_cn=[\"第一\",\"明\",\"美\",\"喜\",\"好\",\"鲜\",\"香\",\"金\",\"银\",\"玉\",\"翡\",\"翠\",\"鼎\",\"御\",\"宝\",\"八\",\"发\",\"8\",\"福\",\"乐\",\"满\",\"祥\",\"嘉\",\"聚\",\"旺\",\"和\",\"盛\",\"富\",\"御\",\"帝\",\"皇\",\"王\",\"鸿\",\"冠\",\"太子\",\"怡\",\"佳\",\"兴\",\"升\",\"锦\",\"雅\",\"顺\",\"来\",\"欢\",\"悦\",\"丰\",\"隆\",\"万\",\"腾\",\"丽\",\"仁\",\"朝代\",\"豪\",\"强\",\"神\",\"仙\",\"珍\",\"荣\",\"靓\",\"超\",\"利\",\"棒\",\"财\",\"师\",\"寿\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we use regex or NER models to detect keywords in a restaurant name to determine which categories it belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de Sichuan House\n",
      "it Mapo Restaurant\n",
      "de New Sichuan\n",
      "de Long's Kitchen\n",
      "da Golden Bowl Restaurant\n",
      "de Asia Kitchen\n",
      "nl Great wok\n",
      "sv China Village\n",
      "de Joy Garden\n",
      "de Yum Yum Kitchen Chinese Cuisine\n",
      "da Golden Horse Restaurant\n",
      "de Bamboo Garden Chinese Restaurant\n",
      "de Cheung Sheng Chinese Restaurant\n",
      "de Oriental Garden\n",
      "nl Golden Phoenix\n",
      "de Dim Sum Box\n",
      "de Golden Inn Restaurant\n",
      "mt Yi Mei\n",
      "id Kang Kang Shau May Restaurant\n",
      "de Jade Garden\n",
      "de Sing Mee Kitchen\n",
      "es Great Taste Chinese Restaurant\n",
      "es Taste Place\n",
      "sv Harbor Village Cuisine\n",
      "de Grandview Buffet\n",
      "de Kuang's Kitchen\n",
      "de Golden Pavilion Restaurant\n",
      "de J & I Chinese Kitchen\n",
      "de 88 No.1 Chinese Kitchen\n",
      "da Golden Palace Restaurant\n",
      "lt Taipei Tokyo\n",
      "da Golden China Restaurant\n",
      "de Golden Dim Sum\n",
      "fr Spiceville\n",
      "es Panda Asian Cuisine\n",
      "fr Tian Tian Lu Chuaner\n",
      "de Lamei Hot Pot\n",
      "id YinTang Spicy Hot Pot\n",
      "da Golden Dragon II\n",
      "ms Hunan Dynasty\n",
      "de AweSum DimSum\n",
      "de Royal Inn Oriental Cuisine\n",
      "de Jade Garden restaurant\n",
      "de Golden Express Chinese Kitchen\n",
      "de Hong Kong Snack House\n",
      "da Golden Dragon\n",
      "de Darien Chop Suey\n",
      "et Goldan Yan Restaurant\n",
      "de New York Canton Lounge\n",
      "da Golden Bridge Chinese Restaurant\n",
      "de China Garden Restaurant\n",
      "fr June Heng Restaurant\n",
      "cs Lucky House Restaurant\n",
      "id Mr.Pang BBQ\n",
      "es Taste King Restaurant\n",
      "de Fortune Garden Chinese Restaurant\n",
      "de Rice Garden Anchorage\n",
      "de Ho Ming Kitchen\n",
      "de Bamboo Garden Asian Grille\n",
      "es Panda To Go\n",
      "de Qin-Fang Garden Restaurant\n",
      "cs Lucky Corner\n",
      "da Golden Wok\n",
      "es Panda III\n",
      "mt Beijing Kitchen\n",
      "de Peking Garden Chinese Restaurant\n",
      "de Szechuan Kitchen\n",
      "nl Golden Garden\n",
      "de Kam Sheng\n",
      "pl Szechuan Express\n",
      "de No.1 Flower Drum House\n",
      "es Asian Paradise\n",
      "it Bistro 1968\n",
      "da China Delight\n",
      "de Great China Kitchen\n",
      "id Hunan Chinese Amory\n",
      "nl Super Garden\n",
      "fr YANHUANG Gourmet\n",
      "pt Hua Rong\n",
      "de China Kitchen\n",
      "de Jia Mei Asian Kitchen\n",
      "id Shanghai Yau Fat Restaurant\n",
      "it Lucky Bento V.I.P. Cuisine\n",
      "de Chengs Oriental Express\n",
      "da Golden Dragon Chinese Restaurant\n",
      "et Jasmine Tea House\n",
      "de Mei Chang Take Out Food\n",
      "id Shang Hai\n",
      "es Red Panda Kit\n",
      "da Red Tiger Dumpling House\n",
      "de Dumpling Kitchen\n",
      "de Taste Of Sichuan Beaverton\n",
      "de Chinese Kitchen\n",
      "ms Mandarin Dynasty\n",
      "tl Shang Artisan Noodle\n",
      "de Wah Chen Kitchen\n",
      "da Golden Ocean Chinese Restaurant\n",
      "mt Beijing Chinese Restaurant\n",
      "de King Garden\n",
      "pl Dynasty Cafe\n",
      "de China Garden\n",
      "da Golden Dragon Restaurant\n",
      "fr Yummy House Gainesville\n",
      "de Peking Garden\n",
      "pt New Char Siu House\n",
      "it Dragon Villa\n",
      "de Imperial Garden Restaurant\n",
      "de Peach Garden\n",
      "fr Ah Chun Shandong Dumplings\n",
      "de Dim Sum Court\n",
      "es Hunan\n",
      "cs Lucky Key & Fuji Ya\n",
      "fr New Gourmet House\n",
      "de Camden Island\n",
      "de QQ Kitchen\n",
      "de Golden China Buffet\n",
      "de Peking Kitchen\n",
      "de Tung Tin Chinese Restaurant\n",
      "de Sichuan Style Restaurant\n",
      "es Panda Palace\n",
      "mt Beijing Cafe\n",
      "de No.1 Kitchen\n",
      "es China Taste Restaurant\n",
      "de Jing Jing Garden\n",
      "mt Beijing On Grove\n",
      "fr Mayflower Chinese Restaurant & Carryout\n",
      "nl Tree Garden\n",
      "it China chef\n",
      "de Sichuan Impression\n",
      "de Mandarin Garden\n",
      "de Hong Kong kitchen(near Westwego)\n",
      "fr Gourmet House\n",
      "de Dim Sum Palace\n",
      "de V Garden\n",
      "id Hunan Garden\n",
      "fr Hui Tou Xiang\n",
      "fr China Gourmet Longmont\n",
      "da Golden Harbor Grill\n",
      "fr Fire Wok Express\n",
      "ro Lulu Chinese Express\n",
      "da Golden China Chinese Restaurant\n",
      "pt Lao Peking Chinese Restaurant\n",
      "de Bamboo Gardens\n",
      "de China Dragon buffet\n",
      "da Golden Palace Seafood Restaurant\n",
      "it Super Dragon\n",
      "fr New China Gourmet\n",
      "es Best BBQ Cantonese Cuisine\n",
      "ms Shihlin Fremont\n",
      "es Star Dragon\n",
      "nl Wonderful House\n",
      "cs Meet Qin Noodle\n",
      "pl NEW CHINA BUFFET\n",
      "sl Nanjing Duck House\n",
      "da Golden Bowl\n",
      "da Wild Ginger\n",
      "nl Chu-Lee Gardens Bradford Chinese Restaurant\n",
      "de Bamboo Garden Restaurant\n",
      "pt Chef Chao Restaurant\n",
      "de Square Garden Chinese Restaurant\n",
      "sk Lilac Blossom (Sky Meadow)\n",
      "de Lin's Kitchen\n",
      "fr Empire King\n",
      "tr Panda Noodle Bar\n",
      "de Panda Garden Buffet\n",
      "de Joy Kitchen\n",
      "mt Beijing Garden\n",
      "tl Kung Pao Wok\n",
      "de Hot Pot Chen\n",
      "es Panda Chinese Restaurant\n",
      "de Tae Heung Gak Chinese Restaurant\n",
      "de Reren Lamen & Bar\n",
      "es Taste King\n",
      "Validation Set: baseline: 85.25%\n",
      "67.0\n",
      "268\n",
      "Precision + Recall + F1 per class:\n",
      "Label 0\n",
      "Precision, Recall, F1: \t84.49\t75.07\t79.5\n",
      "Label 1\n",
      "Precision, Recall, F1: \t12.37\t20.34\t15.38\n",
      "Weighted Accuracy + Precision + Recall + F1:\n",
      "Accuracy, Precision, Recall, F1: 67.0\t73.85\t67.0\t70.05\n",
      "['Romanized', 0.1475, 0.2425, 67.0, 73.85, 67.0, 70.05]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This function returns True if text contains the specified value, Otherwise returns False\"\"\"\n",
    "def classify_research_value(text, keyword_list=None, value_type=None, language = \"English_Name\"):\n",
    "#     print(text)\n",
    "    if value_type in [\"Location\",'Ambiance','Culture','Specialty','Positivity'] or value_type == 'Pun_Creative' and language==\"English_Name\":\n",
    "        pattern = re.compile('|'.join(map(re.escape, keyword_list)), re.IGNORECASE)\n",
    "        if pattern.search(text):\n",
    "            return True\n",
    "        elif re.search(\"\\b(?:new data|new corpus)\", text):\n",
    "            return True\n",
    "    elif value_type == 'Pun_Creative' and language==\"Chinese_Name\":\n",
    "        # this helper function checks if a phrase is similar to a Chinese chengyu\n",
    "        def find_idiom(text,df):\n",
    "            score = process.extract(text, df['word'])[0][0]\n",
    "            try:\n",
    "                return score > 0.8 or score == 0.8\n",
    "            except:\n",
    "                return False\n",
    "        # this helper function checks if a phrase fits a potential pattern of creativity, i.e. having repeated characters.\n",
    "        def find_repetition(text):\n",
    "            d = collections.defaultdict(int)\n",
    "            for c in text:\n",
    "                d[c] += 1\n",
    "            return 2 in d.values()\n",
    "        return find_idiom(text,df_idioms) or find_repetition(text)\n",
    "    elif value_type == \"Personal_Name\":\n",
    "        if language == \"English_Name\":\n",
    "            ner_results = nlp(text)\n",
    "            for item in ner_results:\n",
    "                if \"PER\" in item['entity']:\n",
    "                    return True\n",
    "            return False\n",
    "        elif language == \"Chinese_Name\":\n",
    "            \n",
    "            # helper function\n",
    "            def get_entity(sentence):\n",
    "                tokens = tokenizer_cn.tokenize(sentence)\n",
    "                inputs = tokenizer_cn.encode(sentence, return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    outputs = model_cn(inputs).logits\n",
    "                predictions = torch.argmax(outputs, dim=2)\n",
    "                char_tags = [(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())][1:-1]\n",
    "                print(sentence)\n",
    "                print(char_tags)\n",
    "\n",
    "                pred_labels = [i[1] for i in char_tags]\n",
    "                entities = []\n",
    "                line_entities = get_entities(pred_labels)\n",
    "                for i in line_entities:\n",
    "                    word = sentence[i[1]: i[2] + 1]\n",
    "                    entity_type = i[0]\n",
    "                    entities.append((word, entity_type))\n",
    "\n",
    "                print(\"Sentence entity:\")\n",
    "                print(entities)\n",
    "                return entities\n",
    "            \n",
    "            results = get_entity(text)\n",
    "            for item in results:\n",
    "                if \"PER\" in item[1]:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "    # for Romanized, the keywords_list parameter MUST be LOCATION  \n",
    "    elif value_type == 'Romanized' and language ==\"English_Name\":\n",
    "        output = langid.classify(text)\n",
    "        output = output[0]\n",
    "#         print(output[0][0]['label'])\n",
    "        if not output == 'en':\n",
    "            print(output,text)\n",
    "            is_name = False\n",
    "            # checking if the string is a personal name\n",
    "            ner_results = nlp(text)\n",
    "            for item in ner_results:\n",
    "                if \"PER\" in item['entity']:\n",
    "                    is_name = True\n",
    "            # checking if the string is a location\n",
    "            is_location = False\n",
    "            pattern = re.compile('|'.join(map(re.escape, keyword_list)), re.IGNORECASE)\n",
    "            if pattern.search(text) or re.search(\"\\b(?:new data|new corpus)\", text):\n",
    "                is_location = True\n",
    "            # if the string is neither name nor location then it's probably romanization\n",
    "            return (not is_name) and (not is_location)\n",
    "\n",
    "    return False\n",
    "\n",
    "# this function calculates accuracy and other scores\n",
    "def get_balanced_experiment_score(df_, category):\n",
    "    y_true, y_pred = df_[category].tolist(), df_.predicted_label.tolist()\n",
    "    print(\"Validation Set: baseline: {}%\".format(round((1-df_[category].mean())*100, 2)))\n",
    "    accuracy = round(100*accuracy_score(y_true, y_pred), 2)\n",
    "    print(accuracy)\n",
    "    print(accuracy_score(y_true, y_pred, normalize=False))\n",
    "    precision, recall, f1, _= precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    print(\"Precision + Recall + F1 per class:\")\n",
    "    for i, (p, r, f) in enumerate(zip(precision, recall, f1)):\n",
    "        print(\"Label\", i)\n",
    "        print(\"Precision, Recall, F1: \\t{}\\t{}\\t{}\".format(round(p*100, 2), round(r*100, 2), round(f*100, 2)) )\n",
    "    # print(precision, recall, f1)\n",
    "    print(\"Weighted Accuracy + Precision + Recall + F1:\")\n",
    "    precision, recall, f1, _= precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    print(\"Accuracy, Precision, Recall, F1: {}\\t{}\\t{}\\t{}\".format(accuracy, round(precision*100, 2), round(recall*100, 2), round(f1*100, 2)) )\n",
    "    # commented out for debugging\n",
    "    try:\n",
    "        proportion_pred = df_.predicted_label.value_counts()[1]/df_.predicted_label.count()\n",
    "    except:\n",
    "        print(df_.predicted_label.value_counts())\n",
    "        proportion_pred = 0\n",
    "    return [proportion_pred, accuracy, round(precision*100, 2), round(recall*100, 2), round(f1*100, 2)]\n",
    "\n",
    "\n",
    "# this function assembles the scores and information on the specific language and category and records the scores to a csv file\n",
    "def classifier(category, keywords, source, output_path,language=\"English_Name\"):\n",
    "    df_val = source #pd.read_csv(\"training_validation/training_en.csv\")\n",
    "    df_val['predicted_label'] = df_val[language].apply(classify_research_value, keyword_list = keywords, value_type=category, language = language)\n",
    "    proportion = source[category].value_counts()[1]/source[category].count()\n",
    "    scores = [category,proportion]+get_balanced_experiment_score(df_val, category)\n",
    "    print(scores)\n",
    "    df_output = pd.read_csv(output_path)\n",
    "    df_output.loc[len(df_output.index)] = scores\n",
    "    df_output.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "source_cn = pd.read_csv(\"../data_cleaning/output/validation_cn.csv\")\n",
    "source_en = pd.read_csv(\"../data_cleaning/output/validation_en.csv\")\n",
    "\n",
    "############# Chinese validation\n",
    "# classifier('Location',location_cn, source_cn, 'scores/validation/Regex/cn_training_scores.csv',language=\"Chinese_Name\")\n",
    "# classifier('Ambiance',ambiance_cn, source_cn, 'scores/validation/Regex/cn_training_scores.csv',language=\"Chinese_Name\")\n",
    "# classifier('Specialty',specialty_cn, source_cn, 'scores/validation/Regex/cn_training_scores.csv',language=\"Chinese_Name\")\n",
    "# classifier('Positivity',positivity_cn, source_cn, 'scores/validation/Regex/cn_training_scores.csv',language=\"Chinese_Name\")\n",
    "# classifier('Pun_Creative',creativity, source_cn, 'scores/validation/Regex/cn_training_scores.csv',language=\"Chinese_Name\")\n",
    "# classifier('Culture',culture_cn, source_cn, 'scores/validation/Regex/cn_training_scores.csv',language=\"Chinese_Name\")\n",
    "# classifier('Personal_Name',culture_cn, source_cn, 'scores/validation/Regex/cn_training_scores.csv',language=\"Chinese_Name\")\n",
    "\n",
    "############# English validation\n",
    "# classifier('Location',location, source_en, 'scores/validation/Regex/en_training_scores.csv')\n",
    "# classifier('Ambiance',ambiance, source_en, 'scores/validation/Regex/en_training_scores.csv')\n",
    "# classifier('Specialty',specialty, source_en, 'scores/validation/Regex/en_training_scores.csv')\n",
    "# classifier('Positivity',positivity, source_en, 'scores/validation/Regex/en_training_scores.csv')\n",
    "# classifier('Pun_Creative',creativity, source_en, 'scores/validation/Regex/en_training_scores.csv')\n",
    "# classifier('Culture',culture, source_en, 'scores/validation/Regex/en_training_scores.csv')\n",
    "# classifier('Personal_Name',culture, source_en, 'scores/validation/Regex/en_training_scores.csv')\n",
    "classifier('Romanized',location, source_en, 'scores/validation/Regex/en_training_scores.csv')\n",
    "\n",
    "############# Chinese test\n",
    "\n",
    "############# English test\n",
    "\n",
    "\n",
    "# print(source['Location'].value_counts()[1]/source['Location'].count())\n",
    "# print(source['Location'].value_counts())\n",
    "\n",
    "# print(source['Culture'].value_counts())\n",
    "# print(source['Ambiance'].value_counts())\n",
    "\n",
    "# classifier('Location',location, source, 'scores/Regex/en_training_scores.csv')\n",
    "# classifier('Culture',culture, source.sample(frac=0.15), 'scores/Regex/en_training_scores.csv')\n",
    "# classifier('Culture',culture, source, 'scores/Regex/en_training_scores.csv')\n",
    "# classifier('Ambiance',ambiance, source, 'scores/Regex/en_training_scores.csv')\n",
    "# classifier('Specialty',specialty, source, 'scores/Regex/en_training_scores.csv')\n",
    "# classifier('Positivity',positivity, source, 'scores/Regex/en_training_scores.csv')\n",
    "# classifier('Personal_Name',[], source, 'scores/Regex/en_training_scores.csv')\n",
    "\n",
    "# source['predicted_label'] = source.English_Name.apply(classify_research_value, keyword_list = location, value_type=\"Location\")\n",
    "# get_balanced_experiment_score(source)\n",
    "\n",
    "    # results = pd.DataFrame(columns=['sample_id','national_id','English_Name','Chinese_Name',args.category])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing datasets with outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = pd.read_csv(r'training_validation/testing.csv')\n",
    "\n",
    "# this is a wrapper function for the classify_research_value function that transforms its output from boolean values to numbers\n",
    "def get_result(text, keyword_list, value_type, language):\n",
    "    if classify_research_value(text, keyword_list, value_type, language):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def classify(category, keywords, source, output_path,language=\"English_Name\"):\n",
    "    df_val = source\n",
    "    df_val[category] = df_val[language].apply(get_result, keyword_list = keywords, value_type=category, language = language)\n",
    "    df_val = df_val.drop(columns=['location_link','site','us_state','type','subtypes','Area','rating','has_name','source','chatGPT_translations'])\n",
    "    df_val.drop(columns=['Unnamed: 0'])\n",
    "    df_val.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "# classify(\"Positivity\", positivity_cn, source, \"outputs/Chinese/best/regex_positivity.csv\", language=\"Chinese_Name\")\n",
    "classify(\"Culture\", culture_cn, source, \"outputs/Chinese/best/regex_culture.csv\", language=\"Chinese_Name\")\n",
    "classify(\"Culture\", culture, source, \"outputs/English/best/regex_culture.csv\")\n",
    "classify(\"Ambiance\", ambiance, source, \"outputs/English/best/regex_ambiance.csv\")\n",
    "\n",
    "#     proportion = source[category].value_counts()[1]/source[category].count()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# !python3 -m venv .env\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# !source .env/bin/activate\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39m_version_)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# !conda install pytorch::pytorch torchvision torchaudio -c pytorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# !python3 -m conda install pytorch torchvision -c pytorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# !python3 -m pip install 'transformers[torch]'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# !python3 -version\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# !python3 -m pip install torch torchvision\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# !python3 -m pip install transformers\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# !python3 -m venv .env\n",
    "# !source .env/bin/activate\n",
    "import torch\n",
    "print(torch._version_)\n",
    "# !conda install pytorch::pytorch torchvision torchaudio -c pytorch\n",
    "# !python3 -m conda install pytorch torchvision -c pytorch\n",
    "# !python3 -m pip install 'transformers[torch]'\n",
    "# !python3 -version\n",
    "# !python3 -m pip install torch torchvision\n",
    "# !python3 -m pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese Creative Fuzzy String Matching Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cjkfuzz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcjkfuzz\u001b[39;00m \u001b[39mimport\u001b[39;00m fuzz \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcjkfuzz\u001b[39;00m \u001b[39mimport\u001b[39;00m process\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/model_training/classifier_regex.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_idioms \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mchinese_idioms.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cjkfuzz'"
     ]
    }
   ],
   "source": [
    "from cjkfuzz import fuzz \n",
    "from cjkfuzz import process\n",
    "df_idioms = pd.read_csv('chinese_idioms.csv')\n",
    "import collections\n",
    "# fuzz.ratio(\"天外飞鲜\", \"天外飞仙\")\n",
    "print(process.extract(\"友情客串\", df['word'])[0][0])\n",
    "\n",
    "def find_idiom(text,df):\n",
    "    score = process.extract(text, df['word'])[0][0]\n",
    "    return score > 0.8 or score == 0.8\n",
    "\n",
    "def find_repetition(text):\n",
    "    d = collections.defaultdict(int)\n",
    "    for c in text:\n",
    "        d[c] += 1\n",
    "    return 2 in d.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
