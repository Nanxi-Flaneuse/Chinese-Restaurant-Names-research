{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7be62c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MA.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/data_cleaning/0_clean.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/data_cleaning/0_clean.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcsv\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/data_cleaning/0_clean.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/data_cleaning/0_clean.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mMA.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/data_cleaning/0_clean.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m states \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAlabama\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mArkansas\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mKentucky\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLouisiana\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMississippi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTennessee\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTexas\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOklahoma\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mCalifornia\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOregon\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWashington\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAlaska\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHawaii\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mArizona\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mColorado\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIdaho\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMontana\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNevada\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNew Mexico\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mUtah\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWyoming\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mIllinois\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIndiana\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIowa\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mKansas\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMichigan\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMinnesota\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissouri\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNebraska\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNorth Dakota\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOhio\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSouth Dakota\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWisconsin\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mPennsylvania\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVermont\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWest Virginia\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDistrict Of Columbia\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mConnecticut\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDelaware\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFlorida\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGeorgia\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMaine\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMaryland\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMassachusetts\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNew Hampshire\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNew Jersey\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNew York\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNorth Carolina\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRhode Island\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSouth Carolina\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVirginia\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nanxiliu/Documents/GitHub/Chinese-Restaurant-Names/data_cleaning/0_clean.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m files \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39meast_coast_1.csv\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39meast_coast_2.csv\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmountain_west.csv\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39msouth.csv\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mpacific_west.csv\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmidwest.csv\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_engine(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmemory_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding_errors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MA.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# import openai\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "test = pd.read_csv('MA.csv')\n",
    "states = ['Alabama', 'Arkansas', 'Kentucky', 'Louisiana', 'Mississippi', 'Tennessee', 'Texas', 'Oklahoma','California', 'Oregon', 'Washington', 'Alaska', 'Hawaii','Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming','Illinois', 'Indiana', 'Iowa', 'Kansas', 'Michigan', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'Ohio', 'South Dakota', 'Wisconsin','Pennsylvania', 'Vermont', 'West Virginia','District Of Columbia','Connecticut', 'Delaware', 'Florida', 'Georgia', 'Maine', 'Maryland', 'Massachusetts', 'New Hampshire', 'New Jersey', 'New York', 'North Carolina', 'Rhode Island', 'South Carolina', 'Virginia']\n",
    "files = ['east_coast_1.csv','east_coast_2.csv','mountain_west.csv','south.csv','pacific_west.csv','midwest.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d7f88",
   "metadata": {},
   "source": [
    "## Testing block. Here we test different code and add them to different functios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f711e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1911\n",
      "1811\n"
     ]
    }
   ],
   "source": [
    "pattern  = re.compile(r'\\bfast food|fast-food|fastfood|chain|express\\b', re.I)\n",
    "ind = test[test['subtypes'].str.contains(pattern, na=False)].index\n",
    "new_test = test.drop(ind)\n",
    "ind_1 = new_test[new_test['description'].str.contains(pattern, na=False)].index\n",
    "new_test = new_test.drop(ind_1)\n",
    "print(test.shape[0])\n",
    "print(new_test.shape[0])\n",
    "new_test.to_csv('MA_testing.csv',index=False,encoding='utf-8')\n",
    "# print(ind[ind[:]==True].index)\n",
    "# print(MA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4d017",
   "metadata": {},
   "source": [
    "## Adding information to sampled 4000 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a22f913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/1xc3_hrd4556zhtcl0bbnq9r0000gn/T/ipykernel_2682/4159678023.py:11: DtypeWarning: Columns (35,39,43,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  national = pd.read_csv(r'national.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sample_id   national_id         area       rating     has_name\n",
      "count  4018.000000   4018.000000  4018.000000  4018.000000  4018.000000\n",
      "mean   2015.621205  16649.595819     1.680687     4.039970     0.486809\n",
      "std    1164.164745   9583.567299     0.634130     0.428759     0.499888\n",
      "min       0.000000      0.000000     0.000000     1.000000     0.000000\n",
      "25%    1007.250000   8094.000000     2.000000     3.800000     0.000000\n",
      "50%    2017.500000  16471.500000     2.000000     4.100000     0.000000\n",
      "75%    3023.750000  24548.250000     2.000000     4.300000     1.000000\n",
      "max    4029.000000  33275.000000     3.000000     5.000000     1.000000\n"
     ]
    }
   ],
   "source": [
    "# this function grabs additional information we need for the filtered sample from the original national dataset\n",
    "# info should be a list of categories from the national dataset we need.\n",
    "def get_info(sample, original):\n",
    "    new = sample.merge(original, how='left',on='national_id') #[['sample_id','national_id','name','location_link','site','us_state','type','subtypes','area','rating','has_name','CH_name','source','range','postal_code']]\n",
    "    \n",
    "#     sample.merge(original, left_on='national_id', right_on='national_id')[['sample_id','national_id','name','location_link','site','us_state','type','subtypes','area','rating','has_name','CH_name','source','range','postal_code']]\n",
    "    print(sample.describe())\n",
    "    new.to_csv('merged_sample.csv', index = False, encoding='utf-8')\n",
    "    \n",
    "sample = pd.read_csv(r'sample_original.csv')\n",
    "national = pd.read_csv(r'national.csv')\n",
    "get_info(sample, national)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4151a5",
   "metadata": {},
   "source": [
    "### Separate Chinese restaurants from other businesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0147cb",
   "metadata": {},
   "source": [
    "###### We remove wrong queries from each region. We scraped our data by region, and each region only contains certain states, but sometimes restaurants from some other state appear in a regional dataset, so we need to remove them\n",
    "\n",
    "###### We then removed the businesses that are actually not Chinese restaurants from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c10c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function focuses on a single column and returns the frequencies of the values in a csv\n",
    "def get_freq(file, column,save_to):\n",
    "    \n",
    "    counts = {}\n",
    "    for value in file[column]:\n",
    "        counts[value] = counts.get(value, 0) + 1\n",
    "    with open(save_to,'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(counts.items())\n",
    "    # types.to_csv(path+'types.csv',index=False,encoding='utf-8')\n",
    "\n",
    "# this function returns the total number of items when given a list of datasets\n",
    "def get_total(path,restaurants):\n",
    "    total = 0\n",
    "    for res in restaurants:\n",
    "        df = pd.read_csv(path+res)\n",
    "        total += df.shape[0]\n",
    "    return total\n",
    "\n",
    "#this function assembles the given datasets into one dataset\n",
    "def assemble(path,files,savename):\n",
    "    datasets=[]\n",
    "    for file in files:\n",
    "        df = pd.read_csv(path+file)\n",
    "        datasets.append(df)\n",
    "    df_total = pd.concat(datasets)\n",
    "    # print(df_total['type'].value_counts())\n",
    "    df_total.to_csv(savename,index=False,encoding='utf-8')\n",
    "    # picking out unique value's frequencies in a column\n",
    "    # counts = {}\n",
    "    # for value in df_total['type']:\n",
    "    #     counts[value] = counts.get(value, 0) + 1\n",
    "    # with open(path+'types.csv','w') as f:\n",
    "    #     w = csv.writer(f)\n",
    "    #     w.writerows(counts.items())\n",
    "    # types.to_csv(path+'types.csv',index=False,encoding='utf-8')\n",
    "    # df_total.to_csv(path+'national.csv',index=False,encoding='utf-8')\n",
    "\n",
    "assemble('datasets_chinese_filtered/',files,'national_chinese_res_filtered.csv')\n",
    "# this function filters out businesses from states that don't belogn in a region. eg. MA in west coast\n",
    "def filter_region(csvfile, states):\n",
    "    df = pd.read_csv('raw_datasets/'+ csvfile)\n",
    "    regions = df[df['us_state'].isin(states)]\n",
    "    regions = regions.reset_index(drop = True)\n",
    "    regions.to_csv('raw_datasets/'+csvfile, encoding='utf-8')\n",
    "#     return df\n",
    "\n",
    "\n",
    "east_coast_1 = ['Connecticut', 'Delaware', 'Florida', 'Georgia', 'Maine', 'Maryland', 'Massachusetts', 'New Hampshire', 'New Jersey', 'New York', 'North Carolina', 'Rhode Island', 'South Carolina', 'Virginia']\n",
    "east_coast_2 = ['Pennsylvania', 'Vermont', 'West Virginia','District Of Columbia']\n",
    "midwest = ['Illinois', 'Indiana', 'Iowa', 'Kansas', 'Michigan', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'Ohio', 'South Dakota', 'Wisconsin']\n",
    "mountain_west = ['Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming']\n",
    "pacific_west = ['California', 'Oregon', 'Washington', 'Alaska', 'Hawaii']\n",
    "south = ['Alabama', 'Arkansas', 'Kentucky', 'Louisiana', 'Mississippi', 'Tennessee', 'Texas', 'Oklahoma']\n",
    "\n",
    "states = [east_coast_1, east_coast_2, mountain_west, south, pacific_west, midwest]\n",
    "\n",
    "#execution code\n",
    "# for i in range(6):\n",
    "#     filter_region(files[i],states[i])\n",
    "    \n",
    "# print(get_total('raw_datasets/',files))\n",
    "# print(get_total('datasets_state_cleaned/',files))\n",
    "\n",
    "# this function filters out businesses that are not really chinese restaurants and put the filtered df in separate csv files\n",
    "def filter_business(csvfile):\n",
    "    filepath = 'datasets_state_cleaned/'+csvfile\n",
    "    df = pd.read_csv(filepath)\n",
    "    categories = ['Chinese restaurant', 'Sichuan restaurant', 'Shanghainese restaurant', 'Taiwanese restaurant', 'Hunan restaurant', 'Mandarin restaurant', 'Cantonese restaurant', 'Chinese takeaway', 'Chinese noodle restaurant','Dim sum restaurant','Dumpling restaurant','Delivery Chinese restaurant','Hot pot restaurant']\n",
    "    cn_res = df[df['type'].isin(categories)]\n",
    "    cn_res = cn_res.reset_index(drop = True)\n",
    "#     non_res = df[~ df['type'].isin(categories)]\n",
    "#     non_res = non_res.reset_index(drop = True)\n",
    "#     print(cn_res)\n",
    "    cn_res.to_csv('datasets_chinese_filtered/'+csvfile, index=False,encoding='utf-8')\n",
    "#     non_res.to_csv('filtered_datasets/other_businesses/other_filtered_'+csvfile, encoding='utf-8')\n",
    "\n",
    "#execution code    \n",
    "# for file in files:\n",
    "#     filter_business(file)\n",
    "    \n",
    "# print(get_total('datasets_chinese_filtered/',files))\n",
    "# assemble('datasets_state_cleaned/',files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbaf953",
   "metadata": {},
   "source": [
    "### Drop Duplicates and fast food restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75c8e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/1xc3_hrd4556zhtcl0bbnq9r0000gn/T/ipykernel_1660/2428536484.py:30: DtypeWarning: Columns (60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_from)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31727\n",
      "31548\n"
     ]
    }
   ],
   "source": [
    "# drops duplicate (chain) restaurants\n",
    "# def drop(file):\n",
    "#     # read csv\n",
    "#     df = pd.read_csv('filtered_datasets/restaurants/filtered_'+file)\n",
    "#     new_df = df.drop_duplicates(subset=['name'],keep = False)\n",
    "#     new_df.to_csv('filtered_datasets/classified_res/'+file, encoding='utf-8')\n",
    "# #     return new_df\n",
    "def del_ff(path_from, path_to):\n",
    "    df = pd.read_csv(path_from)\n",
    "    # index_ff = df[df['subtypes'].str.match('fast food',case=False)|df['subtypes'].str.match('express',case=False)|df['description'].str.match('express',case=False)|df['description'].str.match('fast food',case=False)].index\n",
    "    # df = df.drop(index_ff)\n",
    "    # df = df.reset_index(drop = True)\n",
    "    # print(df.shape[0])\n",
    "    print(df.shape[0])\n",
    "    pattern  = re.compile(r'\\bfast food|fast-food|fastfood|chain|express\\b', re.I)\n",
    "    name_pattern = re.compile(r'\\bfast food|fast-food|fastfood\\b', re.I)\n",
    "    ind = df[df['subtypes'].str.contains(pattern, na=False)|df['description'].str.contains(pattern, na=False)|df['reviews_tags'].str.contains(pattern, na=False)|df['name'].str.contains(name_pattern, na=False)].index\n",
    "    df_final = df.drop(ind)\n",
    "    # ind_1 = new_df[new_df['description'].str.contains(pattern, na=False)].index\n",
    "    # df_final = new_df.drop(ind_1)\n",
    "    print(df_final.shape[0])\n",
    "    df_final.to_csv(path_to, index=False, encoding='utf-8')\n",
    "\n",
    "# del_ff('national_chinese_res_filtered.csv','national_no_ff.csv')\n",
    "\n",
    "# drops additional chain restaurants that have not been filtered due to naming\n",
    "def del_chain(path_from, path_to):\n",
    "#     chains = ['Manchu Wok','Asian Chao','City Wok','Chowking','Pick Up Stix','HuHot Mongolian Grill','Panda Express',\"Mama Fu's\",\"BD's Mongolian Grill\",'Leeann Chin',\"Mark Pi's\",'Flat Top Grill','Big Bowl','Din Tai Fung',\"P.F. Chang's\",'Lao Sze Chuan','Pei Wei Asian Kitchen','Mr. Chow','Chinese Gourmet Express']\n",
    "#     df = pd.read_csv('filtered_datasets/restaurants/filtered_'+file)\n",
    "    df = pd.read_csv(path_from)\n",
    "    #  delete chains\n",
    "    pattern  = re.compile(r'\\bB2J Fish Soup|Mama Fu\\'s|Manchu Wok|Happy Lamb|BD\\'s Mongolian Grill|panda express|Mao\\'s Bao|P.F. Chang\\'s|Leeann Chin|Xi\\'an Famous Foods|Asian Chao|Mark Pi\\'s|Ten Seconds Yunnan Rice Noodle|Flat Top Grill|Liuyishou|Big Bowl|A&J Restaurant|Haidilao|Boiling Point|Szechuan Impression|Din Tai Fung|City Wok|Bo Ling\\'s|Lao Sze Chuan|Meizhou Dongpo|Chowking|Pei Wei|Yang\\'s Braised Chicken Rice|Mr. Chow|Tasty Pot|Pick Up Stix|Dumplings of Fury|Chinese Gourmet Express|CHIKO|HuHot Mongolian Grill|Dagu Rice Noodle|ChiliSpot\\b', re.I)\n",
    "    # indexChain = re.compile(r'\\bpanda express\\b', re.I)\n",
    "    # indexChain = df[(df['name'].str.contains(\"PANDA EXPRESS\")) | (df['name'].str.contains(\"Panda express\")) | (df['name'].str.contains(\"Mao's Bao\")) | (df['name'].str.contains(\"P.F. Chang's\")) | (df['name'].str.contains(\"Xi'an Famous Foods\")) | (df['name'].str.contains('Ten Seconds Yunnan Rice Noodle')) | (df['name'].str.contains('Liuyishou')) | (df['name'].str.contains('Haidilao')) | (df['name'].str.contains('Boiling Point')) | (df['name'].str.contains('Szechuan Impression')) | (df['name'].str.contains(\"Bo Ling's\")) | (df['name'].str.contains('Meizhou Dongpo')) | (df['name'].str.contains(\"Yang's Braised Chicken Rice\")) | (df['name'].str.contains('Tasty Pot')) | (df['name'].str.contains('Dumplings of Fury')) | (df['name'].str.contains('CHIKO')) | (df['name'].str.contains('Dagu Rice Noodle')) | (df['name'].str.contains('ChiliSpot')) | (df['name'].str.contains('Happy Lamb')) | (df['name'].str.contains('Manchu Wok')) | (df['name'].str.contains('B2J Fish Soup 不二家酸菜鱼')) | (df['name'].str.contains('Asian Chao')) | (df['name'].str.contains('A&J Restaurant')) | (df['name'].str.contains('City Wok')) |(df['name'].str.contains('Chowking'))| (df['name'].str.contains('Pick Up Stix'))| (df['name'].str.contains('HuHot Mongolian Grill')) | (df['name'].str.contains('Panda Express')) | (df['name'].str.contains(\"Mama Fu's\")) | (df['name'].str.contains(\"BD's Mongolian Grill\")) | (df['name'].str.contains('Leeann Chin')) | (df['name'].str.contains(\"Mark Pi's\")) | (df['name'].str.contains('Flat Top Grill')) | (df['name'].str.contains('Big Bowl')) | (df['name'].str.contains('Din Tai Fung')) | (df['name'].str.contains('Lao Sze Chuan')) | (df['name'].str.contains('Pei Wei')) | (df['name'].str.contains('Mr. Chow')) | (df['name'].str.contains('Chinese Gourmet Express'))].index\n",
    "    indexChain = df[df['name'].str.contains(pattern, na=False)].index\n",
    "    print(df.shape[0])\n",
    "    df = df.drop(indexChain) #, inplace = True\n",
    "#     print(df_filtered.describe())\n",
    "    # deletes fast food\n",
    "    # index_ff = df[df['subtypes'].str.contains('fast food')].index\n",
    "    # indef_FF = df[df['subtypes'].str.contains('Fast food')].index\n",
    "    \n",
    "    # des_chain = df[df['description'].str.match('chain',case=False)].index\n",
    "    # df = df.drop(des_chain)\n",
    "    df = df.reset_index(drop = True)\n",
    "    print(df.shape[0])\n",
    "    df.to_csv(path_to, index=False, encoding='utf-8')\n",
    "del_chain('national_no_ff.csv','national_no_hidden_chain.csv')\n",
    "# files = ['east_coast_1.csv','east_coast_2.csv','east_coast_3.csv','mountain_west.csv','south.csv','west_coast.csv','midwest_1.csv','midwest_2.csv']\n",
    "# for file in files:\n",
    "#     drop(file)\n",
    "# for file in files:\n",
    "#     del_chain(file)\n",
    "# for file in files:\n",
    "#     drop_ff(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044e17d",
   "metadata": {},
   "source": [
    "## Recleaning the 4000 sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ecb174",
   "metadata": {},
   "source": [
    "### Classifying areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea064cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this function returns an area's classification based on its RUCA2 score\n",
    "ruca_df = pd.read_csv(r'RUCA.csv')\n",
    "\n",
    "def get_area(score):\n",
    "    if score in [1,1.1]:   # urban\n",
    "        return 2\n",
    "    elif score in [2, 2.1, 4, 4.1]:  # suburban\n",
    "        return 1\n",
    "    elif score == 999:\n",
    "        return 3\n",
    "    return 0 # rural\n",
    "\n",
    "\n",
    "# returns a RUCA score based on the input postal code\n",
    "def get_ruca(post):\n",
    "    try:\n",
    "        row_ind = ruca_df[ruca_df['ZIP_CODE'] == post].index.values    # is the row index here the real row index or the column row index?\n",
    "        ruca_score = ruca_df.loc[row_ind, 'RUCA2']\n",
    "        return ruca_score.iloc[0]\n",
    "    except:\n",
    "        print('zip code '+str(post)+' not found. Classified as other. Area number = 3')\n",
    "        return 999\n",
    "\n",
    "# returns the number classification of an area based on its postal code\n",
    "def classify(post):\n",
    "#     post = float(post)\n",
    "    return get_area(get_ruca(post))\n",
    "\n",
    "# this classifies each restaurant as urban/rural/suburban\n",
    "# it takes in a file as an input and adds a colum \"Area\" to that file\n",
    "def area_classify(file,new_name):\n",
    "    # read csv\n",
    "    df = pd.read_csv(file)\n",
    "    areas = df['postal_code'].apply(classify)  # apply function on column postal_code\n",
    "    df['Area'] = areas # add new column to dataframe\n",
    "    df.to_csv(new_name,index=False, encoding='utf-8')\n",
    "    return df\n",
    "\n",
    "# files = ['east_coast_1.csv','east_coast_2.csv','east_coast_3.csv','mountain_west.csv','south.csv','west_coast.csv','midwest_1.csv','midwest_2.csv']\n",
    "# for file in files:\n",
    "# file=pd.read_csv('national_no_hidden_chain.csv')\n",
    "area_classify('national_no_hidden_chain.csv','national_region_classified.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c9e6e",
   "metadata": {},
   "source": [
    "### Filtering out datapoints in the buggy sample that exists in the new national dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97a39998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/1xc3_hrd4556zhtcl0bbnq9r0000gn/T/ipykernel_2580/4006033773.py:1: DtypeWarning: Columns (60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nn_df = pd.read_csv('national_region_classified.csv')\n"
     ]
    }
   ],
   "source": [
    "nn_df = pd.read_csv('national_region_classified.csv')\n",
    "bug_df = pd.read_csv('sample_full_info.csv')\n",
    "clean_df = bug_df.google_id.isin(nn_df.google_id)\n",
    "bug_df['in_national'] = clean_df\n",
    "bug_df = bug_df[bug_df.in_national == True]\n",
    "bug_df.to_csv('sample_cleaned.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982a63e",
   "metadata": {},
   "source": [
    "### Comparing the state numbers between old and new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6080239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/1xc3_hrd4556zhtcl0bbnq9r0000gn/T/ipykernel_1304/3083790615.py:3: DtypeWarning: Columns (60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nn_df = pd.read_csv('national_region_classified.csv')\n"
     ]
    }
   ],
   "source": [
    "clean_df = pd.read_csv('sample_cleaned.csv')\n",
    "old_df = pd.read_csv('sample_full_info.csv')\n",
    "nn_df = pd.read_csv('national_region_classified.csv')\n",
    "# print(clean_df['us_state'].value_counts())\n",
    "states_new = clean_df['us_state'].value_counts()\n",
    "states_old = old_df['us_state'].value_counts()\n",
    "# print(states_new)\n",
    "# print(states_old)\n",
    "# get_freq(clean_df,'us_state','clean_states.csv')\n",
    "# get_freq(old_df,'us_state','old_states.csv')\n",
    "\n",
    "get_freq(nn_df,'us_state','all_states.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3cd78",
   "metadata": {},
   "source": [
    "### Calculating the percentage of state sample frequencies to national fequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "878957ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         state   percentage\n",
      "0  Connecticut           52\n",
      "1     Delaware           47\n",
      "2      Florida          157\n",
      "3      Georgia           87\n",
      "4        Maine           46\n",
      "           state   percentage\n",
      "0       Virginia          907\n",
      "1       New York         2979\n",
      "2        Georgia          899\n",
      "3       Delaware          138\n",
      "4  New Hampshire          177\n"
     ]
    }
   ],
   "source": [
    "sample_state = pd.read_csv('clean_states.csv') \n",
    "national_state = pd.read_csv('all_states.csv') \n",
    "print(sample_state.head())\n",
    "print(national_state.head())\n",
    "\n",
    "### code for merging\n",
    "new = sample_state.merge(national_state, how='left',on='state') #[['sample_id','national_id','name','location_link','site','us_state','type','subtypes','area','rating','has_name','CH_name','source','range','postal_code']]\n",
    "new.to_csv('merged_state_proportion.csv', index = False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7d726",
   "metadata": {},
   "source": [
    "### Expired: Compile National df with filtered Chinese restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b21798cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36424\n"
     ]
    }
   ],
   "source": [
    "def national_df(file):\n",
    "    df = pd.read_csv('datasets_chinese_filtered/'+file)\n",
    "#     df = pd.read_csv('filtered_datasets/restaurants/filtered_'+file) \n",
    "    return df\n",
    "    \n",
    "def get_national():\n",
    "    files = ['east_coast_1.csv','east_coast_2.csv','mountain_west.csv','south.csv','pacific_west.csv','midwest.csv']\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = national_df(file) #.dropna(subset=['cid', 'reviews_id'])\n",
    "        # print(df['cid'].dtype)\n",
    "        df['cid'] = df['cid'].astype('string')\n",
    "        df['reviews_id'] = df['reviews_id'].astype('string',errors='ignore')\n",
    "        # df['reviews_id'] = df['reviews_id'].astype('int64',errors='ignore')\n",
    "        dfs.append(df)\n",
    "    national = pd.concat(dfs).reset_index(drop = True)\n",
    "    print(national.shape[0])\n",
    "    # national['reviews_id'].astype(int,errors='ignore')\n",
    "#     national = national.drop_duplicates(subset=['name'],keep = False)\n",
    "#     national[national.duplicated(subset=['name'],keep=False)].reset_index(drop = True).to_csv('filtered_datasets/classified_res/duplicates.csv', encoding='utf-8')\n",
    "    national.to_csv('output/national_testing.csv', encoding='utf-8')\n",
    "#     return national\n",
    "\n",
    "get_national()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc2611",
   "metadata": {},
   "source": [
    "### Making state dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3644aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "national =  pd.read_csv(r'filtered_datasets/classified_res/national_nochains.csv')\n",
    "def state_df(state):\n",
    "    df = national[national['us_state'].isin([state])]\n",
    "    df.to_csv('filtered_datasets/states/'+state+'.csv', encoding='utf-8')\n",
    "def get_states():\n",
    "    for state in states:\n",
    "        state_df(state)\n",
    "get_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbccaa",
   "metadata": {},
   "source": [
    "### Count # restaurants in each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30315b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19788\n"
     ]
    }
   ],
   "source": [
    "def df_count(file):\n",
    "    df = pd.read_csv('filtered_datasets/classified_res/'+file)\n",
    "    return df.count()[0]\n",
    "total = 0\n",
    "for file in files:\n",
    "    total += df_count(file)\n",
    "print(total)\n",
    "# print(df_count('east_coast_1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb63845",
   "metadata": {},
   "source": [
    "### Categories of non-Chinese-restaurant businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d669c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_categories(filename):\n",
    "    df = pd.read_csv('filtered_datasets/other_businesses/other_filtered_'+ filename)\n",
    "#     types = df['type'].unique()\n",
    "    n = 10\n",
    "    print(filename)\n",
    "    print(df['type'].value_counts()[:n].index.tolist())\n",
    "#     print(df['type'].describe())\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "files = ['east_coast_1.csv','east_coast_2.csv','east_coast_3.csv','mountain_west.csv','south.csv','west_coast.csv','midwest_1.csv','midwest_2.csv']\n",
    "# for file in files:\n",
    "#     get_categories(file)\n",
    "# get_categories('east_coast_1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
