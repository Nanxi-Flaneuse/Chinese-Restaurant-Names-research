{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7be62c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "states = ['Alabama', 'Arkansas', 'Kentucky', 'Louisiana', 'Mississippi', 'Tennessee', 'Texas', 'Oklahoma','California', 'Oregon', 'Washington', 'Alaska', 'Hawaii','Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming','Illinois', 'Indiana', 'Iowa', 'Kansas', 'Michigan', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'Ohio', 'South Dakota', 'Wisconsin','Pennsylvania', 'Vermont', 'West Virginia','District Of Columbia','Connecticut', 'Delaware', 'Florida', 'Georgia', 'Maine', 'Maryland', 'Massachusetts', 'New Hampshire', 'New Jersey', 'New York', 'North Carolina', 'Rhode Island', 'South Carolina', 'Virginia']\n",
    "files = ['east_coast_1.csv','east_coast_2.csv','east_coast_3.csv','mountain_west.csv','south.csv','west_coast.csv','midwest_1.csv','midwest_2.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4d017",
   "metadata": {},
   "source": [
    "## Adding information to sampled 4000 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function grabs additional information we need for the filtered sample from the original national dataset\n",
    "# info should be a list of categories from the national dataset we need.\n",
    "def get_info(sample, original):\n",
    "    new = sample.merge(original, how='left',on='national_id') #[['sample_id','national_id','name','location_link','site','us_state','type','subtypes','area','rating','has_name','CH_name','source','range','postal_code']]\n",
    "    \n",
    "#     sample.merge(original, left_on='national_id', right_on='national_id')[['sample_id','national_id','name','location_link','site','us_state','type','subtypes','area','rating','has_name','CH_name','source','range','postal_code']]\n",
    "    print(sample.describe())\n",
    "    new.to_csv('merged_sample.csv', index = False, encoding='utf-8')\n",
    "    \n",
    "sample = pd.read_csv(r'sample_original.csv')\n",
    "national = pd.read_csv(r'national.csv')\n",
    "get_info(sample, national)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4151a5",
   "metadata": {},
   "source": [
    "### Separate Chinese restaurants from other businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c10c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function filters out businesses from states that don't belogn in a region. eg. MA in west coast\n",
    "def filter_region(csvfile, states):\n",
    "    df = pd.read_csv('raw_datasets/'+ csvfile)\n",
    "    regions = df[df['us_state'].isin(states)]\n",
    "    regions = regions.reset_index(drop = True)\n",
    "    regions.to_csv('filtered_datasets/filtered_regions/region_fltr_'+csvfile, encoding='utf-8')\n",
    "#     return df\n",
    "\n",
    "\n",
    "east_coast_1 = ['Connecticut', 'Delaware', 'Florida', 'Georgia', 'Maine', 'Maryland', 'Massachusetts', 'New Hampshire', 'New Jersey', 'New York', 'North Carolina', 'Rhode Island', 'South Carolina', 'Virginia']\n",
    "east_coast_2 = ['Pennsylvania', 'Vermont', 'West Virginia','District Of Columbia']\n",
    "midwest = ['Illinois', 'Indiana', 'Iowa', 'Kansas', 'Michigan', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'Ohio', 'South Dakota', 'Wisconsin']\n",
    "mountain_west = ['Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming']\n",
    "west_coast = ['California', 'Oregon', 'Washington', 'Alaska', 'Hawaii']\n",
    "south = ['Alabama', 'Arkansas', 'Kentucky', 'Louisiana', 'Mississippi', 'Tennessee', 'Texas', 'Oklahoma']\n",
    "\n",
    "states = [east_coast_1, east_coast_2, east_coast_1, mountain_west, south, west_coast, midwest, midwest]\n",
    "for i in range(8):\n",
    "    filter_region(files[i],states[i])\n",
    "    \n",
    "\n",
    "# this function filters out businesses that are not really chinese restaurants and put the filtered df in separate csv files\n",
    "def filter_business(csvfile):\n",
    "    filepath = 'filtered_datasets/filtered_regions/region_fltr_'+csvfile\n",
    "    df = pd.read_csv(filepath)\n",
    "    categories = ['Chinese restaurant', 'Sichuan restaurant', 'Shanghainese restaurant', 'Taiwanese restaurant', 'Hunan restaurant', 'Mandarin restaurant', 'Cantonese restaurant', 'Chinese takeaway', 'Chinese noodle restaurant','Dim sum restaurant','Dumpling restaurant','Delivery Chinese restaurant','Hot pot restaurant']\n",
    "    cn_res = df[df['type'].isin(categories)]\n",
    "    cn_res = cn_res.reset_index(drop = True)\n",
    "#     non_res = df[~ df['type'].isin(categories)]\n",
    "#     non_res = non_res.reset_index(drop = True)\n",
    "#     print(cn_res)\n",
    "    cn_res.to_csv('filtered_datasets/restaurants/filtered_'+csvfile, encoding='utf-8')\n",
    "#     non_res.to_csv('filtered_datasets/other_businesses/other_filtered_'+csvfile, encoding='utf-8')\n",
    "    \n",
    "for file in files:\n",
    "    filter_business(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbaf953",
   "metadata": {},
   "source": [
    "### Drop Duplicates and fast food restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c8e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops duplicate (chain) restaurants\n",
    "# def drop(file):\n",
    "#     # read csv\n",
    "#     df = pd.read_csv('filtered_datasets/restaurants/filtered_'+file)\n",
    "#     new_df = df.drop_duplicates(subset=['name'],keep = False)\n",
    "#     new_df.to_csv('filtered_datasets/classified_res/'+file, encoding='utf-8')\n",
    "# #     return new_df\n",
    "\n",
    "# drops additional chain restaurants that have not been filtered due to naming\n",
    "def del_chain(file):\n",
    "#     chains = ['Manchu Wok','Asian Chao','City Wok','Chowking','Pick Up Stix','HuHot Mongolian Grill','Panda Express',\"Mama Fu's\",\"BD's Mongolian Grill\",'Leeann Chin',\"Mark Pi's\",'Flat Top Grill','Big Bowl','Din Tai Fung',\"P.F. Chang's\",'Lao Sze Chuan','Pei Wei Asian Kitchen','Mr. Chow','Chinese Gourmet Express']\n",
    "#     df = pd.read_csv('filtered_datasets/restaurants/filtered_'+file)\n",
    "    df = pd.read_csv('filtered_datasets/classified_res/'+file)\n",
    "    #  delete chains\n",
    "    indexChain = df[(df['name'].str.contains(\"PANDA EXPRESS\")) | (df['name'].str.contains(\"Panda express\")) | (df['name'].str.contains(\"Mao's Bao\")) | (df['name'].str.contains(\"P.F. Chang's\")) | (df['name'].str.contains(\"Xi'an Famous Foods\")) | (df['name'].str.contains('Ten Seconds Yunnan Rice Noodle')) | (df['name'].str.contains('Liuyishou')) | (df['name'].str.contains('Haidilao')) | (df['name'].str.contains('Boiling Point')) | (df['name'].str.contains('Szechuan Impression')) | (df['name'].str.contains(\"Bo Ling's\")) | (df['name'].str.contains('Meizhou Dongpo')) | (df['name'].str.contains(\"Yang's Braised Chicken Rice\")) | (df['name'].str.contains('Tasty Pot')) | (df['name'].str.contains('Dumplings of Fury')) | (df['name'].str.contains('CHIKO')) | (df['name'].str.contains('Dagu Rice Noodle')) | (df['name'].str.contains('ChiliSpot')) | (df['name'].str.contains('Happy Lamb')) | (df['name'].str.contains('Manchu Wok')) | (df['name'].str.contains('B2J Fish Soup 不二家酸菜鱼')) | (df['name'].str.contains('Asian Chao')) | (df['name'].str.contains('A&J Restaurant')) | (df['name'].str.contains('City Wok')) |(df['name'].str.contains('Chowking'))| (df['name'].str.contains('Pick Up Stix'))| (df['name'].str.contains('HuHot Mongolian Grill')) | (df['name'].str.contains('Panda Express')) | (df['name'].str.contains(\"Mama Fu's\")) | (df['name'].str.contains(\"BD's Mongolian Grill\")) | (df['name'].str.contains('Leeann Chin')) | (df['name'].str.contains(\"Mark Pi's\")) | (df['name'].str.contains('Flat Top Grill')) | (df['name'].str.contains('Big Bowl')) | (df['name'].str.contains('Din Tai Fung')) | (df['name'].str.contains('Lao Sze Chuan')) | (df['name'].str.contains('Pei Wei')) | (df['name'].str.contains('Mr. Chow')) | (df['name'].str.contains('Chinese Gourmet Express'))].index\n",
    "    df = df.drop(indexChain) #, inplace = True\n",
    "#     print(df_filtered.describe())\n",
    "    # deletes fast food\n",
    "    index_ff = df[df['subtypes'].str.contains('fast food')].index\n",
    "    df = df.drop(index_ff)\n",
    "    df = df.reset_index(drop = True)\n",
    "    df.to_csv('filtered_datasets/classified_res/'+file, encoding='utf-8')\n",
    "\n",
    "files = ['east_coast_1.csv','east_coast_2.csv','east_coast_3.csv','mountain_west.csv','south.csv','west_coast.csv','midwest_1.csv','midwest_2.csv']\n",
    "# for file in files:\n",
    "#     drop(file)\n",
    "for file in files:\n",
    "    del_chain(file)\n",
    "# for file in files:\n",
    "#     drop_ff(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ecb174",
   "metadata": {},
   "source": [
    "### Classifying areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea064cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip code 12026 not found. Classified as other. Area number = 3\n",
      "zip code 89542 not found. Classified as other. Area number = 3\n",
      "zip code 91719 not found. Classified as other. Area number = 3\n"
     ]
    }
   ],
   "source": [
    "# this function returns an area's classification based on its RUCA2 score\n",
    "ruca_df = pd.read_csv(r'RUCA.csv')\n",
    "\n",
    "def get_area(score):\n",
    "    if score in [1,1.1]:   # urban\n",
    "        return 2\n",
    "    elif score in [2, 2.1, 4, 4.1]:  # suburban\n",
    "        return 1\n",
    "    elif score == 999:\n",
    "        return 3\n",
    "    return 0 # rural\n",
    "\n",
    "\n",
    "# returns a RUCA score based on the input postal code\n",
    "def get_ruca(post):\n",
    "    try:\n",
    "        row_ind = ruca_df[ruca_df['ZIP_CODE'] == post].index.values    # is the row index here the real row index or the column row index?\n",
    "        ruca_score = ruca_df.loc[row_ind, 'RUCA2']\n",
    "        return ruca_score.iloc[0]\n",
    "    except:\n",
    "        print('zip code '+str(post)+' not found. Classified as other. Area number = 3')\n",
    "        return 999\n",
    "\n",
    "# returns the number classification of an area based on its postal code\n",
    "def classify(post):\n",
    "#     post = float(post)\n",
    "    return get_area(get_ruca(post))\n",
    "\n",
    "# this classifies each restaurant as urban/rural/suburban\n",
    "# it takes in a file as an input and adds a colum \"Area\" to that file\n",
    "def area_classify(file):\n",
    "    # read csv\n",
    "    df = pd.read_csv('filtered_datasets/classified_res/'+file)\n",
    "    areas = df['postal_code'].apply(classify)  # apply function on column postal_code\n",
    "    df['Area'] = areas # add new column to dataframe\n",
    "    df.to_csv('filtered_datasets/classified_res/'+file, encoding='utf-8')\n",
    "    return df\n",
    "\n",
    "files = ['east_coast_1.csv','east_coast_2.csv','east_coast_3.csv','mountain_west.csv','south.csv','west_coast.csv','midwest_1.csv','midwest_2.csv']\n",
    "for file in files:\n",
    "    area_classify(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7d726",
   "metadata": {},
   "source": [
    "### Compile National df with filtered Chinese restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b21798cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33277\n"
     ]
    }
   ],
   "source": [
    "def national_df(file):\n",
    "    df = pd.read_csv('filtered_datasets/classified_res/'+file)\n",
    "#     df = pd.read_csv('filtered_datasets/restaurants/filtered_'+file) \n",
    "    return df\n",
    "    \n",
    "def get_national():\n",
    "    files = ['east_coast_1.csv','east_coast_2.csv','east_coast_3.csv','mountain_west.csv','south.csv','west_coast.csv','midwest_1.csv','midwest_2.csv']\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        dfs.append(national_df(file))\n",
    "    national = pd.concat(dfs).reset_index(drop = True)\n",
    "    print(national.shape[0])\n",
    "#     national = national.drop_duplicates(subset=['name'],keep = False)\n",
    "#     national[national.duplicated(subset=['name'],keep=False)].reset_index(drop = True).to_csv('filtered_datasets/classified_res/duplicates.csv', encoding='utf-8')\n",
    "    national.to_csv('filtered_datasets/classified_res/national_nochains.csv', encoding='utf-8')\n",
    "#     return national\n",
    "\n",
    "get_national()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc2611",
   "metadata": {},
   "source": [
    "### Making state dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3644aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "national =  pd.read_csv(r'filtered_datasets/classified_res/national_nochains.csv')\n",
    "def state_df(state):\n",
    "    df = national[national['us_state'].isin([state])]\n",
    "    df.to_csv('filtered_datasets/states/'+state+'.csv', encoding='utf-8')\n",
    "def get_states():\n",
    "    for state in states:\n",
    "        state_df(state)\n",
    "get_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbccaa",
   "metadata": {},
   "source": [
    "### Count # restaurants in each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30315b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19788\n"
     ]
    }
   ],
   "source": [
    "def df_count(file):\n",
    "    df = pd.read_csv('filtered_datasets/classified_res/'+file)\n",
    "    return df.count()[0]\n",
    "total = 0\n",
    "for file in files:\n",
    "    total += df_count(file)\n",
    "print(total)\n",
    "# print(df_count('east_coast_1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb63845",
   "metadata": {},
   "source": [
    "### Categories of non-Chinese-restaurant businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d669c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "east_coast_1.csv\n",
      "['Restaurant', 'Asian restaurant', 'Thai restaurant', 'American restaurant', 'Japanese restaurant', 'Seafood restaurant', 'Pizza restaurant', 'Sushi restaurant', 'Asian fusion restaurant', 'Buffet restaurant']\n",
      "--------------------------------------------------------------------\n",
      "east_coast_2.csv\n",
      "['Restaurant', 'Postal code', 'Pizza restaurant', 'American restaurant', 'Thai restaurant', 'Asian restaurant', 'Japanese restaurant', 'Family restaurant', 'Bar & grill', 'Sushi restaurant']\n",
      "--------------------------------------------------------------------\n",
      "east_coast_3.csv\n",
      "['Restaurant', 'Asian restaurant', 'Thai restaurant', 'Japanese restaurant', 'American restaurant', 'Asian fusion restaurant', 'Pizza restaurant', 'Sushi restaurant', 'Seafood restaurant', 'Buffet restaurant']\n",
      "--------------------------------------------------------------------\n",
      "mountain_west.csv\n",
      "['Restaurant', 'Thai restaurant', 'American restaurant', 'Mexican restaurant', 'Asian restaurant', 'Vietnamese restaurant', 'Japanese restaurant', 'Sushi restaurant', 'Pizza restaurant', 'Asian fusion restaurant']\n",
      "--------------------------------------------------------------------\n",
      "south.csv\n",
      "['Restaurant', 'Mexican restaurant', 'American restaurant', 'Thai restaurant', 'Japanese restaurant', 'Postal code', 'Asian restaurant', 'Fast food restaurant', 'Vietnamese restaurant', 'Pizza restaurant']\n",
      "--------------------------------------------------------------------\n",
      "west_coast.csv\n",
      "['Restaurant', 'Thai restaurant', 'Vietnamese restaurant', 'Japanese restaurant', 'Asian restaurant', 'Asian fusion restaurant', 'Sushi restaurant', 'American restaurant', 'Mexican restaurant', 'Korean restaurant']\n",
      "--------------------------------------------------------------------\n",
      "midwest_1.csv\n",
      "['Restaurant', 'American restaurant', 'Bar & grill', 'Pizza restaurant', 'Thai restaurant', 'Postal code', 'Asian restaurant', 'Japanese restaurant', 'Family restaurant', 'Bar']\n",
      "--------------------------------------------------------------------\n",
      "midwest_2.csv\n",
      "['Restaurant', 'Bar & grill', 'American restaurant', 'Thai restaurant', 'Pizza restaurant', 'Mexican restaurant', 'Postal code', 'Bar', 'Asian restaurant', 'Fast food restaurant']\n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_categories(filename):\n",
    "    df = pd.read_csv('filtered_datasets/other_businesses/other_filtered_'+ filename)\n",
    "#     types = df['type'].unique()\n",
    "    n = 10\n",
    "    print(filename)\n",
    "    print(df['type'].value_counts()[:n].index.tolist())\n",
    "#     print(df['type'].describe())\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "files = ['east_coast_1.csv','east_coast_2.csv','east_coast_3.csv','mountain_west.csv','south.csv','west_coast.csv','midwest_1.csv','midwest_2.csv']\n",
    "# for file in files:\n",
    "#     get_categories(file)\n",
    "# get_categories('east_coast_1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
