{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84cb8f1d",
   "metadata": {},
   "source": [
    "### Importing libraries and initializing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f7b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import multiprocessing as mp\n",
    "from bert_score import score\n",
    "from pprint import pprint\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "import evaluate\n",
    "from evaluate import load \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# def cap(x):\n",
    "#     return x.title()\n",
    "\n",
    "# new_df = df.applymap(cap)\n",
    "# print(new_df)\n",
    "# new_df.to_csv('all_translations_cap.csv',index='False',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285ea1e",
   "metadata": {},
   "source": [
    "### A.  Load new translations into all_translations_scores.csv to prepare for score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee4b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(r'all_translations_scores.csv')\n",
    "except:\n",
    "    df = pd.read_csv(r'../outputs/translation_all.csv')\n",
    "\n",
    "def add_score(translation, file_to, col_name):\n",
    "    df_trans = pd.read_csv('../outputs/'+translation)\n",
    "    df_to = pd.read_csv('../score_calculation/'+file_to)\n",
    "    df_to[col_name] = df_trans[col_name].to_numpy()\n",
    "    df_to.to_csv('../score_calculation/'+file_to)\n",
    "# add_score('gpt4_7_new.csv','all_translations_scores.csv','gpt4_7_new')\n",
    "\n",
    "add_score('gpt4_0_new.csv','all_translations_scores.csv','gpt4_0_new')\n",
    "add_score('gpt4_1_new.csv','all_translations_scores.csv','gpt4_1_new')\n",
    "add_score('gpt4_2_new.csv','all_translations_scores.csv','gpt4_2_new')\n",
    "add_score('gpt4_3_new.csv','all_translations_scores.csv','gpt4_3_new')\n",
    "add_score('gpt4_4_new.csv','all_translations_scores.csv','gpt4_4_new')\n",
    "add_score('gpt4_5_new.csv','all_translations_scores.csv','gpt4_5_new')\n",
    "add_score('gpt4_6_new.csv','all_translations_scores.csv','gpt4_6_new')\n",
    "add_score('gpt4_8_new.csv','all_translations_scores.csv','gpt4_8_new')\n",
    "add_score('gpt4_9_new.csv','all_translations_scores.csv','gpt4_9_new')\n",
    "add_score('gpt4_10_new.csv','all_translations_scores.csv','gpt4_10_new')\n",
    "add_score('gpt4_11_new.csv','all_translations_scores.csv','gpt4_11_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca97b7",
   "metadata": {},
   "source": [
    "### B. helper functions for various score calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9dda95",
   "metadata": {},
   "source": [
    "#### B.1 Calculating consine similarity using transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2babf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Two lists of sentences\n",
    "def semantic_compare(data, filename=\"all_translations_scores.csv\", key1 = \"English_Name\", key2 = \"gpt35_translation\"):\n",
    "    \n",
    "    names1 = data[key1] #\"English_Name\"\n",
    "\n",
    "    names2 = data[key2] #chatGPT_translation\n",
    "#     print(\"names1 dtype\", names1.dtypes)\n",
    "\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(names1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(names2, convert_to_tensor=True)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    scores = []\n",
    "    #Output the pairs with their score\n",
    "    for i in range(len(names1)):\n",
    "        cos = cosine_scores[i][i]\n",
    "        scores.append(\"{:.4f}\".format(cos))\n",
    "    #     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(names1[i], names2[i], cos))\n",
    "    key = key1+key2+'consine'\n",
    "    data[key] = scores\n",
    "    data.to_csv(filename, index=False,encoding='utf-8')\n",
    "    data[key] = pd.to_numeric(data[key], downcast='float')\n",
    "    print(\"Naming semantics similarity:\",data[key].mean())\n",
    "\n",
    "# try:\n",
    "#     df = pd.read_csv(r'all_translations_scores.csv')\n",
    "# except:\n",
    "#     df = pd.read_csv(r'../outputs/translation_all.csv')\n",
    "\n",
    "# semantic_compare(df,\"all_translations_scores.csv\",'nanxi_translation')\n",
    "# semantic_compare(df,\"all_translations_scores.csv\",'rukun_translation')\n",
    "# semantic_compare(df,\"all_translations_scores.csv\",'nanxi_translation','gpt4_translation')\n",
    "# semantic_compare(df,\"all_translations_scores.csv\",'rukun_translation','gpt4_translation')\n",
    "# semantic_compare(df,\"all_translations_scores.csv\",'rukun_translation','gpt4_0')\n",
    "# semantic_compare(df,\"all_translations_scores.csv\",'nanxi_translation','gpt4_0')\n",
    "# semantic_compare(CH_df,\"rural_embedding_translation_literal.csv\",key2 = \"Literal_Translation\")\n",
    "# semantic_compare(CH_urban_df,\"urban_embedding_translation_literal.csv\",key2 = \"Literal_Translation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc2bfb",
   "metadata": {},
   "source": [
    "#### B.2 Evaluating text similarities with BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c6c91c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# When you are running this cell for the first time, \n",
    "# it will download the BERT model which will take relatively longer. \n",
    "# P, R, F1 = score(cands, refs, lang=\"en\", verbose=True)\n",
    "# print(f\"System level F1 score: {F1.mean():.3f}\")\n",
    "\n",
    "def BERT_compare(data, filename,key1 = \"English_Name\", key2 = \"gpt35_translation\"):\n",
    "    \n",
    "    names1 = data[key1].tolist() #\"English_Name\"\n",
    "\n",
    "    names2 = data[key2].tolist() #chatGPT_translation\n",
    "  \n",
    "    bertscore = BERTScore()\n",
    "    # printing F1 scores\n",
    "    scores = bertscore(names1, names2)\n",
    "    F1 = scores['f1'].tolist()\n",
    "    precision = scores['precision'].tolist()\n",
    "    recall = scores['recall'].tolist()\n",
    "    data[key1+key2+'BERT_F1'] = F1\n",
    "    data[key1+key2+'BERT_precision'] = precision\n",
    "    data[key1+key2+'BERT_recall'] = recall\n",
    "    data.to_csv(filename, index=False,encoding='utf-8')\n",
    "    # printing mean F1 scores and all the other scores\n",
    "#     print(f\"System level F1 score: {F1.mean():.3f}\")\n",
    "#     pprint(bertscore(names1, names2))\n",
    "# df = pd.read_csv(r'all_translations_scores.csv')\n",
    "# BERT_compare(df,\"all_translations_scores.csv\",'nanxi_translation')\n",
    "# BERT_compare(df,\"all_translations_scores.csv\",'rukun_translation')\n",
    "# BERT_compare(df,\"all_translations_scores.csv\",'nanxi_translation','gpt4_translation')\n",
    "# BERT_compare(df,\"all_translations_scores.csv\",'rukun_translation','gpt4_translation')\n",
    "# BERT_compare(df,\"all_translations_scores.csv\",'nanxi_translation','gpt4_0')\n",
    "# BERT_compare(df,\"all_translations_scores.csv\",'rukun_translation','gpt4_0')\n",
    "# !pip install torchmetrics\n",
    "# from pprint import pprint\n",
    "# from torchmetrics.text.bert import BERTScore\n",
    "# preds = [\"hello there\", \"general kenobi\"]\n",
    "# target = [\"hello there\", \"master kenobi\"]\n",
    "# bertscore = BERTScore()\n",
    "# pprint(bertscore(preds, target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ec09d",
   "metadata": {},
   "source": [
    "#### B.3 Evaluating text similarities with ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0edae7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# source: https://huggingface.co/spaces/evaluate-metric/rouge\n",
    "\n",
    "def ROUGE_compare(data, filename, key1 = \"name\", key2 = \"gpt35_translation\"):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    references = [x.lower() for x in data[key1].tolist()] #\"English_Name\"\n",
    "    predictions = [x.lower() for x in data[key2].tolist()] #chatGPT_translation\n",
    "    scores = rouge.compute(predictions=predictions, references=references, use_aggregator=False)\n",
    "    data[key1+key2+'rouge1'] = scores['rouge1']\n",
    "    data[key1+key2+'rouge2'] = scores['rouge2']\n",
    "    data[key1+key2+'rougeL'] = scores['rougeL']\n",
    "    data[key1+key2+'rougeLsum'] = scores['rougeLsum']\n",
    "    data.to_csv(filename,index=False, encoding='utf-8')\n",
    "# df = pd.read_csv(r'all_translations_scores.csv')\n",
    "# ROUGE_compare(df,\"all_translations_scores.csv\",'nanxi_translation')\n",
    "# ROUGE_compare(df,\"all_translations_scores.csv\",'rukun_translation')\n",
    "# ROUGE_compare(df,\"all_translations_scores.csv\",'nanxi_translation','gpt4_translation')\n",
    "# ROUGE_compare(df,\"all_translations_scores.csv\",'rukun_translation','gpt4_translation')\n",
    "\n",
    "# ROUGE_compare(df,\"all_translations_scores.csv\",'nanxi_translation','gpt4_0')\n",
    "# ROUGE_compare(df,\"all_translations_scores.csv\",'rukun_translation','gpt4_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f131a4",
   "metadata": {},
   "source": [
    "#### B.4 Evaluating text similarities with BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b340cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://huggingface.co/spaces/evaluate-metric/bleu\n",
    "# !pip install bleu\n",
    "def BLEU_compare(data, filename, key1 = \"name\", key2 = \"gpt35_translation\"):\n",
    "    bleu = evaluate.load('bleu')\n",
    "    references = [x.lower() for x in data[key1].tolist()] #\"English_Name\"\n",
    "    predictions = [x.lower() for x in data[key2].tolist()] #chatGPT_translation\n",
    "    res = []\n",
    "    for i in range(len(references)):\n",
    "        res.append(bleu.compute(predictions=[predictions[i]], references=[references[i]])['bleu'])\n",
    "    scores = bleu.compute(predictions=predictions, references=references) #, tokenizer=word_tokenize)\n",
    "    print(scores)\n",
    "    data[key1+key2+'bleu'] = res\n",
    "    data.to_csv(filename,index=False, encoding='utf-8')\n",
    "    \n",
    "# df = pd.read_csv(r'all_translations_scores.csv')\n",
    "\n",
    "# BLEU_compare(df,\"all_translations_scores.csv\",'nanxi_translation')\n",
    "# BLEU_compare(df,\"all_translations_scores.csv\",'rukun_translation')\n",
    "# BLEU_compare(df,\"all_translations_scores.csv\",'nanxi_translation','gpt4_translation')\n",
    "# BLEU_compare(df,\"all_translations_scores.csv\",'rukun_translation','gpt4_translation')\n",
    "# BLEU_compare(df,\"all_translations_scores.csv\",'nanxi_translation','gpt4_0')\n",
    "# BLEU_compare(df,\"all_translations_scores.csv\",'rukun_translation','gpt4_0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b35a3",
   "metadata": {},
   "source": [
    "### C. executing all the score calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0acfc89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naming semantics similarity: 0.748861\n",
      "Naming semantics similarity: 0.761592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanxiliu/anaconda3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The argument `model_name_or_path` was not specified while it is required when the default `transformers` model is used. It will use the default recommended model - 'roberta-large'.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nanxiliu/anaconda3/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The argument `model_name_or_path` was not specified while it is required when the default `transformers` model is used. It will use the default recommended model - 'roberta-large'.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.31072100320984514, 'precisions': [0.5659163987138264, 0.3791469194312796, 0.23893805309734514, 0.18181818181818182], 'brevity_penalty': 1.0, 'length_ratio': 1.0436241610738255, 'translation_length': 311, 'reference_length': 298}\n",
      "{'bleu': 0.25591972990551154, 'precisions': [0.5659163987138264, 0.33649289099526064, 0.17699115044247787, 0.12727272727272726], 'brevity_penalty': 1.0, 'length_ratio': 1.0798611111111112, 'translation_length': 311, 'reference_length': 288}\n"
     ]
    }
   ],
   "source": [
    "def get_scores(col):\n",
    "    df = pd.read_csv(r'all_translations_scores.csv')\n",
    "    semantic_compare(df,\"all_translations_scores.csv\",'rukun_translation',col)\n",
    "    semantic_compare(df,\"all_translations_scores.csv\",'nanxi_translation',col)\n",
    "\n",
    "    df = pd.read_csv(r'all_translations_scores.csv')\n",
    "    BERT_compare(df,\"all_translations_scores.csv\",'nanxi_translation',col)\n",
    "    BERT_compare(df,\"all_translations_scores.csv\",'rukun_translation',col)\n",
    "    \n",
    "    df = pd.read_csv(r'all_translations_scores.csv')\n",
    "    ROUGE_compare(df,\"all_translations_scores.csv\",'nanxi_translation',col)\n",
    "    ROUGE_compare(df,\"all_translations_scores.csv\",'rukun_translation',col)\n",
    "\n",
    "    df = pd.read_csv(r'all_translations_scores.csv')\n",
    "    BLEU_compare(df,\"all_translations_scores.csv\",'nanxi_translation',col)\n",
    "    BLEU_compare(df,\"all_translations_scores.csv\",'rukun_translation',col)\n",
    "\n",
    "# replace the argument in the get_scores function with the column you're hoping to get the scores from\n",
    "# get_scores('gpt4_0_new')\n",
    "# get_scores('gpt4_1_new')\n",
    "# get_scores('gpt4_2_new')\n",
    "# get_scores('gpt4_3_new')\n",
    "# get_scores('gpt4_4_new')\n",
    "# get_scores('gpt4_5_new')\n",
    "# get_scores('gpt4_6_new')\n",
    "# get_scores('gpt4_8_new')\n",
    "# get_scores('gpt4_9_new')\n",
    "# get_scores('gpt4_10_new')\n",
    "get_scores('gpt4_11_new')\n",
    "# get_scores('gpt4_7_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89518912",
   "metadata": {},
   "source": [
    "### D. getting score averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "961070b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Unnamed: 0.26                            49.500000\n",
      "Unnamed: 0.25                            49.500000\n",
      "Unnamed: 0.24                            49.500000\n",
      "Unnamed: 0.23                            49.500000\n",
      "Unnamed: 0.22                            49.500000\n",
      "                                           ...    \n",
      "rukun_translationgpt4_11_newrouge2        0.361508\n",
      "rukun_translationgpt4_11_newrougeL        0.571465\n",
      "rukun_translationgpt4_11_newrougeLsum     0.571465\n",
      "nanxi_translationgpt4_11_newbleu          0.070000\n",
      "rukun_translationgpt4_11_newbleu          0.023866\n",
      "Length: 495, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(r'all_translations_scores.csv')\n",
    "except:\n",
    "    df = pd.read_csv(r'../outputs/translation_all.csv')\n",
    "\n",
    "mean = df.mean(axis = 0, numeric_only = 1)\n",
    "print(type(mean))\n",
    "print(mean)\n",
    "\n",
    "mean.to_csv('mean.csv',index=False,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
